{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution notebook\n",
    "This notebook serves as a walk-through of the code to execute training of the ViT keypoint tracker using models from the TIMM library on huggingface. This script largely replicates the preprocessing blocks from the Project_execution.ipynb script written to work with the jeonsworld ViT code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare root directory\n",
    "\n",
    "#Mac\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-timm\"\n",
    "\n",
    "#Windows\n",
    "#root = r\"C:\\Users\\avs20\\Documents\\GitHub\\ViT_facemap\\ViT-timm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17935b275f7408e82aa37a56f740c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Project Name:', placeholder='Enter your project name')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the root directory for projects\n",
    "root = os.getcwd()  # Set to current working directory or customize it\n",
    "projects_dir = os.path.join(root, \"projects\")\n",
    "\n",
    "# Create the 'projects' folder if it doesn't exist\n",
    "if not os.path.exists(projects_dir):\n",
    "    os.makedirs(projects_dir)\n",
    "\n",
    "# Function to create a project folder inside 'projects'\n",
    "def create_project_folder(project_name):\n",
    "    # Ensure a valid project name is provided\n",
    "    if project_name.strip() == \"\":\n",
    "        print(\"Please enter a valid project name.\")\n",
    "        return\n",
    "    \n",
    "    # Create the project folder path\n",
    "    project_folder = os.path.join(projects_dir, project_name)\n",
    "    \n",
    "    # Check if the folder already exists\n",
    "    if not os.path.exists(project_folder):\n",
    "        os.makedirs(project_folder)\n",
    "        print(f\"Project folder '{project_folder}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Project folder '{project_folder}' already exists.\")\n",
    "    \n",
    "    # Return the path to the project folder\n",
    "    return project_folder\n",
    "\n",
    "# Create widgets for project name input and button\n",
    "project_name_input = widgets.Text(\n",
    "    description=\"Project Name:\",\n",
    "    placeholder=\"Enter your project name\",\n",
    ")\n",
    "\n",
    "# Display the input box and button\n",
    "display(project_name_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project folder '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-timm/projects/Facemap' already exists.\n"
     ]
    }
   ],
   "source": [
    "project_dict = create_project_folder(project_name_input.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Test-train split (incl. dropping NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain_test_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m split_and_organize_data\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to process and combine data from multiple folders\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m split_and_organize_data(base_dir, output_dir)\n",
      "File \u001b[0;32m~/Documents/GitHub/ViT_facemap/ViT-timm/utils/train_test_split.py:63\u001b[0m, in \u001b[0;36msplit_and_organize_data\u001b[0;34m(base_dir, output_dir, train_size)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(source_image_path):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Copy the image with the prefixed name to the temporary folder\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     dest_image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(temp_folder, image_name)\n\u001b[0;32m---> 63\u001b[0m     shutil\u001b[38;5;241m.\u001b[39mcopy(source_image_path, dest_image_path)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_image_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/shutil.py:419\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(dst):\n\u001b[1;32m    418\u001b[0m     dst \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(src))\n\u001b[0;32m--> 419\u001b[0m copyfile(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    420\u001b[0m copymode(src, dst, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/shutil.py:262\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _HAS_FCOPYFILE:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         _fastcopy_fcopyfile(fsrc, fdst, posix\u001b[38;5;241m.\u001b[39m_COPYFILE_DATA)\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m _GiveupOnFastCopy:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/shutil.py:97\u001b[0m, in \u001b[0;36m_fastcopy_fcopyfile\u001b[0;34m(fsrc, fdst, flags)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _GiveupOnFastCopy(err)  \u001b[38;5;66;03m# not a regular file\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     posix\u001b[38;5;241m.\u001b[39m_fcopyfile(infd, outfd, flags)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     99\u001b[0m     err\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mname\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "base_dir = os.path.join(root, \"data\", \"facemap_multi_video\")\n",
    "output_dir = os.path.join(project_dict, \"data\")\n",
    "\n",
    "from utils.train_test_split import split_and_organize_data\n",
    "\n",
    "# Call the function to process and combine data from multiple folders\n",
    "split_and_organize_data(base_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "- rotation = how much rotation (degrees) should be applied to the image\n",
    "- img_height = input image height (consider changing this to automatically be derived from meta data files if expecting it not to be uniform)\n",
    "- img_size = size in pixels (ViT expect 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to be applied, and input parameters to the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "rescale = transforms.Compose([\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to associate names with each transformation\n",
    "transforms_dict = {\n",
    "    #'rotate_rescale': rotate_rescale,\n",
    "    #'flip_rescale': flip_rescale,\n",
    "    #'pad_rescale': pad_rescale,\n",
    "    #'rotate_flip_rescale': rotate_flip_rescale,\n",
    "    #'blur': blur,\n",
    "    'rescale': rescale\n",
    "}\n",
    "\n",
    "\n",
    "# Import the AugmentedFaceDataset class\n",
    "from utils.Dataaugmentation import AugmentedFaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create augmented train set\n",
    "\n",
    "# Define the paths\n",
    "source_folder = os.path.join(project_dict, \"data\")\n",
    "\n",
    "# Define paths using os.path.join for consistency\n",
    "train_csv_file = os.path.join(source_folder, \"train\", \"train_data.csv\")\n",
    "train_folder = os.path.join(source_folder, \"train\")\n",
    "train_output_dir = os.path.join(source_folder, \"train\", \"augmented_data\")\n",
    "\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=train_csv_file, root_dir=train_folder, output_dir=train_output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create augmented test set\n",
    "\n",
    "# Define the paths\n",
    "source_folder = os.path.join(project_dict, \"data\")\n",
    "\n",
    "# Define paths using os.path.join for consistency\n",
    "test_csv_file = os.path.join(source_folder, \"test\", \"test_data.csv\")\n",
    "test_folder = os.path.join(source_folder, \"test\")\n",
    "test_output_dir = os.path.join(source_folder, \"test\", \"augmented_data\")\n",
    "\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=test_csv_file, root_dir=test_folder, output_dir=test_output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to train and test augmented data\n",
    "# Train\n",
    "train_output_dir = os.path.join(source_folder, \"train\", \"augmented_data\")\n",
    "train_csv = os.path.join(train_output_dir, \"augmented_labels.csv\")\n",
    "\n",
    "# Test\n",
    "test_output_dir = os.path.join(source_folder, \"test\", \"augmented_data\")\n",
    "test_csv = os.path.join(test_output_dir, \"augmented_labels.csv\")\n",
    "\n",
    "print(train_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device, save_dir, test_csv_file, patience=5):\n",
    "    model.to(device)  # Move model to the appropriate device (CPU or GPU)\n",
    "    \n",
    "    # Initialize lists to store the losses for plotting later\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    # Initialize variables for early stopping\n",
    "    best_val_loss = float('inf')  # Best validation loss starts at infinity\n",
    "    epochs_no_improve = 0         # Counter for how many epochs since the last improvement\n",
    "    early_stop = False            # Flag to indicate early stopping\n",
    "    \n",
    "    # Create save directory if it doesn't exist\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    loss_curve_path = os.path.join(save_dir, \"lossCurve.csv\")\n",
    "    \n",
    "    # If the CSV file already exists, we read the data in case the training was interrupted\n",
    "    if os.path.exists(loss_curve_path):\n",
    "        loss_data = pd.read_csv(loss_curve_path)\n",
    "        training_losses = loss_data['training_loss'].tolist()\n",
    "        validation_losses = loss_data['validation_loss'].tolist()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if early_stop:\n",
    "            print(\"Early stopping triggered. Ending training.\")\n",
    "            break\n",
    "\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Get the total number of batches\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_loader):\n",
    "            labels = labels.view(20, -1)  # TODO: Adjust batch size dynamically if needed\n",
    "            images, labels = images.to(device), labels.to(device)  # Move to device\n",
    "            \n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            outputs = model(images)  # Forward pass\n",
    "            loss = criterion(outputs, labels)  # Calculate loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "\n",
    "            # Print step progress\n",
    "            if step % 10 == 0 or step == total_batches - 1:  # Print every 10 steps and the last step\n",
    "                print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{step + 1}/{total_batches}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        training_losses.append(epoch_loss)  # Log training loss\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Average Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Validation step\n",
    "        if test_loader is not None:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            val_loss = 0.0\n",
    "            all_outputs = []  # List to hold outputs\n",
    "            image_names = []  # List to hold image names\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for step, (images, labels) in enumerate(test_loader):\n",
    "                    labels = labels.view(images.size(0), -1)  # Dynamically reshape based on batch size\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    \n",
    "                    # Get the predictions from the model\n",
    "                    outputs = model(images)\n",
    "                    all_outputs.append(outputs.cpu().detach().numpy())  # Collect predictions\n",
    "\n",
    "                    # Assuming test_csv_file contains the image names corresponding to the test set\n",
    "                    if step == 0:  # Assuming the same image names for every step; typically you would read it outside the loop\n",
    "                        image_names.extend(pd.read_csv(test_csv_file)[\"image_name\"].tolist())\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)  # Accumulate loss\n",
    "            \n",
    "            val_loss /= len(test_loader.dataset)\n",
    "            validation_losses.append(val_loss)  # Log validation loss\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # Save outputs to CSV after processing all batches\n",
    "            all_outputs = np.vstack(all_outputs)  # Stack all outputs into a single array\n",
    "            d_preds = pd.DataFrame(all_outputs)  # Convert outputs to DataFrame\n",
    "            d_preds[\"image_names\"] = image_names  # Add image names to DataFrame\n",
    "\n",
    "            # Save DataFrame to CSV\n",
    "            predictions_csv_path = os.path.join(save_dir, \"predictions.csv\")\n",
    "            d_preds.to_csv(predictions_csv_path, index=False)\n",
    "            print(f\"Validation outputs saved to {predictions_csv_path}\")\n",
    "\n",
    "            # Early stopping logic\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss  # Update the best validation loss\n",
    "                epochs_no_improve = 0     # Reset the counter when improvement occurs\n",
    "                # Save the model whenever the validation loss improves\n",
    "                best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"Validation loss improved. Model saved at {best_model_path}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"No improvement in validation loss for {epochs_no_improve} epochs.\")\n",
    "            \n",
    "            # Check if we should stop early\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs without improvement.\")\n",
    "                early_stop = True\n",
    "\n",
    "        # Save the loss curve after each epoch\n",
    "        loss_data = pd.DataFrame({\n",
    "            'epoch': range(1, len(training_losses) + 1),\n",
    "            'training_loss': training_losses,\n",
    "            'validation_loss': validation_losses\n",
    "        })\n",
    "        loss_data.to_csv(loss_curve_path, index=False)\n",
    "        print(f\"Loss curves updated and saved at {loss_curve_path}\")\n",
    "\n",
    "        # Save the model after every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            save_path = os.path.join(save_dir, f\"model_epoch_{epoch + 1}.pth\")\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved at: {save_path}\")\n",
    "\n",
    "    print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up model specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from timm import create_model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from data_utils_timm import get_loader\n",
    "\n",
    "# Set your paths and parameters directly\n",
    "current_dir = os.getcwd()\n",
    "train_csv_file = os.path.join(current_dir,\"projects/Facemap/data/train/augmented_data/augmented_labels.csv\")\n",
    "train_data_dir = os.path.join(current_dir, \"projects/Facemap/data/train/augmented_data\")\n",
    "test_csv_file = os.path.join(current_dir,\"projects/Facemap/data/test/augmented_data/augmented_labels.csv\")\n",
    "test_data_dir = os.path.join(current_dir, \"projects/Facemap/data/test/augmented_data\")\n",
    "save_dir = os.path.join(current_dir, \"projects/Facemap/data/output\")\n",
    "train_batch_size = 20\n",
    "eval_batch_size = 20\n",
    "num_epochs = 300  # Set your desired number of epochs\n",
    "\n",
    "# Initialize the DataLoader\n",
    "train_loader, test_loader = get_loader(train_csv_file, train_data_dir, test_csv_file, test_data_dir, train_batch_size, eval_batch_size)\n",
    "\n",
    "# Create the model\n",
    "#model = create_model('vit_base_r50_s16_224', pretrained=True)\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "# Modify the last layer for regression\n",
    "num_keypoints = 12\n",
    "num_coordinates = num_keypoints * 2\n",
    "\n",
    "# Check the current head of the model\n",
    "print(model.head)\n",
    "\n",
    "# Replace the head appropriately\n",
    "if isinstance(model.head, nn.Identity):\n",
    "    # Directly replace with a new linear layer\n",
    "    model.head = nn.Linear(model.num_features, num_coordinates)  # Use num_features instead of in_features\n",
    "else:\n",
    "    # This handles the cases where the head is not Identity\n",
    "    model.head = nn.Linear(model.head.in_features, num_coordinates)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device, save_dir=save_dir, test_csv_file=test_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=1000, bias=True)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_epoch_100.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m     model\u001b[38;5;241m.\u001b[39mhead \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(model\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39min_features, num_coordinates)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Load the trained model\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_epoch_100.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Check if GPU is available and move model to GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_epoch_100.pth'"
     ]
    }
   ],
   "source": [
    "from timm import create_model\n",
    "import torch.nn as nn\n",
    "# Create the model\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "import torch\n",
    "\n",
    "# Modify the last layer for regression\n",
    "#num_keypoints = 12\n",
    "#num_coordinates = num_keypoints * 2\n",
    "#model.head = nn.Linear(model.head.in_features, num_coordinates)\n",
    "\n",
    "#hybrid model?\n",
    "#model = create_model('vit_base_r50_s16_224', pretrained=True)\n",
    "\n",
    "# Modify the last layer for regression\n",
    "num_keypoints = 12\n",
    "num_coordinates = num_keypoints * 2\n",
    "\n",
    "# Check the current head of the model\n",
    "print(model.head)\n",
    "\n",
    "# Replace the head appropriately\n",
    "if isinstance(model.head, nn.Identity):\n",
    "    # Directly replace with a new linear layer\n",
    "    model.head = nn.Linear(model.num_features, num_coordinates)  # Use num_features instead of in_features\n",
    "else:\n",
    "    # This handles the cases where the head is not Identity\n",
    "    model.head = nn.Linear(model.head.in_features, num_coordinates)\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load('model_epoch_100.pth'))\n",
    "\n",
    "print(model)\n",
    "\n",
    "# Check if GPU is available and move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)  # Move model to the appropriate device\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform (assuming this is required elsewhere)\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Uncomment if resizing is needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Define the data path in an OS-independent way\n",
    "data_path = Path(\"projects\") / \"Facemap\" / \"data\" / \"test\" / \"augmented_data\"\n",
    "\n",
    "# Define the image name\n",
    "\n",
    "#imageName = 'cam1_G7c1_1_img0703_pad_rescale_augmented.jpg'\n",
    "#imageName = 'cam1_G7c1_1_img0703_rotate_rescale_augmented.jpg'\n",
    "imageName = 'cam1_G7c1_1_img0703_flip_rescale_augmented.jpg'\n",
    "\n",
    "\n",
    "# Open the image (using the / operator to concatenate the path and file name)\n",
    "im = Image.open(data_path / imageName)\n",
    "\n",
    "x = transform(im)\n",
    "\n",
    "#print(x)[0]\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv(data_path /'augmented_labels.csv')\n",
    "labelsKepoints = labels.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:]\n",
    "\n",
    "pred_path = Path(\"projects\") / \"Facemap\" / \"data\" / \"output\"\n",
    "preds = pd.read_csv(pred_path /'predictions.csv')\n",
    "predsKeypoints = preds.loc[labels['image_name'] == imageName].values.flatten().tolist()[0:-1]\n",
    "\n",
    "print(predsKeypoints)\n",
    "\n",
    "import torch\n",
    "\n",
    "# Assuming 'transform' and 'model' are defined somewhere in your code\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load and transform the image\n",
    "x = transform(im).to(device)  # Move the transformed image to the appropriate device\n",
    "\n",
    "# Move the model to the same device\n",
    "model.to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get model output\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    out = model(x.unsqueeze(0))  # Ensure input is batched\n",
    "    #attention_weights = model.get_attention_maps(x.unsqueeze(0))\n",
    "\n",
    "#print(out.shape)\n",
    "# Convert the model output to a format suitable for plotting\n",
    "keypoints = out[0].detach().cpu().numpy()  # Detach and move to CPU if using a GPU\n",
    "\n",
    "# Access the first row of keypoints since shape is (1, 24)\n",
    "#keypoints = keypoints[0]\n",
    "\n",
    "print(keypoints)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.imshow(im)\n",
    "# Loop through the keypoints and plot them\n",
    "for i in range(0, len(keypoints), 2):\n",
    "    x_coord = keypoints[i]\n",
    "    y_coord = keypoints[i + 1]\n",
    "    plt.scatter(x_coord, y_coord, s=10, c='blue', marker='x')  # Plot each keypoint\n",
    "for i in range(0, 23, 2):\n",
    "    plt.plot(labelsKepoints[i], labelsKepoints[i+1], 'ro')\n",
    "    plt.plot(predsKeypoints[i], predsKeypoints[i+1], 'yo', markerfacecolor='none', markersize=10) #allows us to plot x+y coordinate of each key point (i+1) and loops over the 24 keypoints, skipping every second step since we plot both x+y \n",
    "\n",
    "plt.title(\"Image with Key Points\")\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the loss curve data\n",
    "losscurve_path = Path(\"projects\") / \"Facemap\" / \"data\" / \"output\"\n",
    "\n",
    "# Load the loss data from CSV\n",
    "d_lossCurve = pd.read_csv(losscurve_path / 'lossCurve.csv')\n",
    "\n",
    "# Define colors for different metrics\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  \n",
    "labels2 = {'validation_loss': 'Test Loss', 'training_loss': 'Training Loss'}  # Rename metrics\n",
    "\n",
    "# Plot each metric\n",
    "for metric, color in colors.items():\n",
    "    steps = d_lossCurve['epoch']  # Extract the epoch column\n",
    "    loss = d_lossCurve[metric]     # Extract the corresponding loss values\n",
    "\n",
    "    # Plot the loss curve\n",
    "    plt.plot(steps, loss, label=labels2[metric], color=color)\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to logarithmic scale for better visualization\n",
    "plt.yscale('log')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
