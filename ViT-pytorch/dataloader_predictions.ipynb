{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Visualization_functions as v\n",
    "\n",
    "v.predictions_formatting()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Predictions_file_remapping' from 'Visualization_functions' (/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Visualization_functions/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms, utils\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mVisualization_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Predictions_file_remapping\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Ignore warnings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Predictions_file_remapping' from 'Visualization_functions' (/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Visualization_functions/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from Visualization_functions import Predictions_file_remapping\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "landmarks_frame = pd.read_csv('predictions.csv')\n",
    "\n",
    "df = v.predictions_formatting(landmarks_frame)\n",
    "print(df)\n",
    "\n",
    "\n",
    "#root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n",
    "#df = pd.read_csv(f\"{root}/predictions.csv\")\n",
    "\n",
    "n = 10\n",
    "img_name = landmarks_frame.iloc[n, 0] #for this to work, remember to take the predictions.csv, and move the image names to the first column (column zero) and delete the last image name column afterwards.\n",
    "landmarks = landmarks_frame.iloc[n, 1:]\n",
    "landmarks = np.asarray(landmarks, dtype=float).reshape(-1, 2)\n",
    "\n",
    "print('Image name: {}'.format(img_name))\n",
    "print('Landmarks shape: {}'.format(landmarks.shape))\n",
    "print('First 4 Landmarks: {}'.format(landmarks[:4]))\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "def show_landmarks(image, landmarks):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.imshow(image)\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], s=10, marker='.', c='r')\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "plt.figure()\n",
    "show_landmarks(io.imread(os.path.join('augmented_data_test/', img_name)),\n",
    "               landmarks)\n",
    "plt.show()\n",
    "\n",
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks], dtype=float).reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "face_dataset = FaceLandmarksDataset(csv_file=\"augmented_data_test/augmented_labels.csv\",\n",
    "                                           root_dir=\"augmented_data_test/\")\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i, sample in enumerate(face_dataset):\n",
    "    print(i, sample['image'].shape, sample['landmarks'].shape)\n",
    "\n",
    "    plt.tight_layout()\n",
    "   # ax.set_title('Sample #{}'.format(i))\n",
    "   # ax.axis('off')\n",
    "    show_landmarks(**sample)\n",
    "\n",
    "    if i == 20:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   image_names           0          1  \\\n",
      "0         img8504_rotate_rescale_augmented.jpg  127.890470  34.085510   \n",
      "1           img8504_flip_rescale_augmented.jpg   74.086590  43.827663   \n",
      "2            img8504_pad_rescale_augmented.jpg  151.806840  43.604350   \n",
      "3    img8504_rotate_flip_rescale_augmented.jpg   54.724567  61.175690   \n",
      "4                   img8504_blur_augmented.jpg  152.059110  43.832943   \n",
      "..                                         ...         ...        ...   \n",
      "135       img7060_rotate_rescale_augmented.jpg  129.536090  34.551490   \n",
      "136         img7060_flip_rescale_augmented.jpg   71.079350  44.908424   \n",
      "137          img7060_pad_rescale_augmented.jpg  153.746540  43.256220   \n",
      "138  img7060_rotate_flip_rescale_augmented.jpg   51.224957  61.456596   \n",
      "139                 img7060_blur_augmented.jpg  154.131090  43.403460   \n",
      "\n",
      "              2          3           4          5           6         7  \\\n",
      "0    112.507420  52.356075  133.692720  61.531197  148.640620  50.57062   \n",
      "1     95.267070  55.494877   78.119560  71.772310   59.941734  66.65037   \n",
      "2    131.182500  55.382390  148.607570  71.050630  166.644490  65.86103   \n",
      "3     78.323100  64.753650   66.939280  86.013565   48.085003  87.36308   \n",
      "4    131.444370  55.595905  148.895600  71.356000  166.925900  66.18925   \n",
      "..          ...        ...         ...        ...         ...       ...   \n",
      "135  112.698326  52.079890  134.280440  62.449760  150.159130  51.93920   \n",
      "136   92.896460  55.588867   76.666336  72.548120   58.209280  68.47281   \n",
      "137  132.114850  54.459053  148.855100  71.372970  167.203000  66.53942   \n",
      "138   74.831100  64.585754   65.348700  85.864190   46.842000  87.71952   \n",
      "139  132.445710  54.627200  149.256350  71.573370  167.660190  66.74309   \n",
      "\n",
      "              8  ...          14         15          16         17  \\\n",
      "0     30.262177  ...   77.998764  99.589600   77.643600  88.382706   \n",
      "1    185.446240  ...  141.929000  88.787240  139.204560  77.514640   \n",
      "2     39.623096  ...   82.980260  89.377850   86.167366  78.256615   \n",
      "3    166.736070  ...  132.845890  81.181564  127.119270  71.097090   \n",
      "4     39.965830  ...   83.396126  89.612976   86.528280  78.454710   \n",
      "..          ...  ...         ...        ...         ...        ...   \n",
      "135   34.292877  ...   86.529760  97.651054   83.135025  86.209145   \n",
      "136  178.379260  ...  134.244460  88.427376  132.813570  77.139570   \n",
      "137   48.035633  ...   93.766210  87.910060   93.756400  76.302925   \n",
      "138  157.037260  ...  125.460710  81.165760  120.219710  71.143130   \n",
      "139   47.904040  ...   93.692760  88.133550   93.774030  76.511930   \n",
      "\n",
      "             18          19          20          21          22          23  \n",
      "0     83.721860  111.113510   88.990555  139.770310  100.878480  143.355910  \n",
      "1    141.422040  100.834430  147.274260  130.212250  136.908660  138.028670  \n",
      "2     84.955246  101.485374   80.672020  129.399580   90.903564  136.810170  \n",
      "3    136.015840   92.196170  151.342650  117.342094  143.771800  128.248610  \n",
      "4     85.341330  101.733770   81.057450  129.739880   91.308240  137.197460  \n",
      "..          ...         ...         ...         ...         ...         ...  \n",
      "135   88.175040  108.483055   88.568520  138.974810  101.340380  143.179670  \n",
      "136  136.159790   99.940500  145.535550  128.039220  135.513490  136.134430  \n",
      "137   90.916985   98.991615   80.415085  128.907000   91.070390  136.947360  \n",
      "138  130.601850   91.654526  149.049260  115.677376  142.433240  126.531944  \n",
      "139   90.957790   99.264206   80.574340  129.186140   91.253525  137.244460  \n",
      "\n",
      "[140 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "import Visualization_functions\n",
    "from Visualization_functions.Predictions_file_remapping import predictions_formatting\n",
    "\n",
    "landmarks_frame = pd.read_csv('predictions.csv')\n",
    "\n",
    "df = predictions_formatting(landmarks_frame)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = [1,2]\n",
    "labels = [2,2]\n",
    "\n",
    "np.mean(preds == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tensor_a = torch.tensor([1, 2, 3, 4, 5])\n",
    "torch.argmax(tensor_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
