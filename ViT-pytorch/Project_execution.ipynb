{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution notebook\n",
    "This notebook serves as a walk-through of the code to execute training of the ViT keypoint tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare root directory\n",
    "\n",
    "#Mac\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n",
    "\n",
    "#Windows\n",
    "#root = r\"C:\\Users\\avs20\\Documents\\GitHub\\ViT_facemap\\ViT-pytorch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download pretrained models (if not already downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from utils.pretrained_model_downloader import download_model\n",
    "\n",
    "# Available model names based on the options in the pretrained_model_downloader script\n",
    "available_models = [\n",
    "    \"R50+ViT-B_16\", \n",
    "    \"ViT-B_16-224\", \n",
    "    \"ViT-B_16\", \n",
    "    \"ViT-B_32\", \n",
    "    \"ViT-B_8\", \n",
    "    \"ViT-L_16-224\", \n",
    "    \"ViT-L_16\", \n",
    "    \"ViT-L_32\"\n",
    "]\n",
    "\n",
    "# Create a dropdown menu widget\n",
    "model_dropdown = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=\"ViT-B_16\",  # Default selected model\n",
    "    description='Choose Model:',\n",
    ")\n",
    "\n",
    "# Display the dropdown\n",
    "display(model_dropdown)\n",
    "\n",
    "# Define a variable to hold the selected model name\n",
    "model_name = model_dropdown.value\n",
    "\n",
    "# Function to update the model_name when a new option is selected\n",
    "def on_model_change(change):\n",
    "    global model_name\n",
    "    model_name = change['new']\n",
    "    print(f\"Selected model: {model_name}\")\n",
    "\n",
    "# Attach the on_model_change function to the dropdown\n",
    "model_dropdown.observe(on_model_change, names='value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = download_model(model_name, root)\n",
    "\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Define the root directory for projects\n",
    "root = os.getcwd()  # Set to current working directory or customize it\n",
    "projects_dir = os.path.join(root, \"projects\")\n",
    "\n",
    "# Create the 'projects' folder if it doesn't exist\n",
    "if not os.path.exists(projects_dir):\n",
    "    os.makedirs(projects_dir)\n",
    "\n",
    "# Function to create a project folder inside 'projects'\n",
    "def create_project_folder(project_name):\n",
    "    # Ensure a valid project name is provided\n",
    "    if project_name.strip() == \"\":\n",
    "        print(\"Please enter a valid project name.\")\n",
    "        return\n",
    "    \n",
    "    # Create the project folder path\n",
    "    project_folder = os.path.join(projects_dir, project_name)\n",
    "    \n",
    "    # Check if the folder already exists\n",
    "    if not os.path.exists(project_folder):\n",
    "        os.makedirs(project_folder)\n",
    "        print(f\"Project folder '{project_folder}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Project folder '{project_folder}' already exists.\")\n",
    "    \n",
    "    # Return the path to the project folder\n",
    "    return project_folder\n",
    "\n",
    "# Create widgets for project name input and button\n",
    "project_name_input = widgets.Text(\n",
    "    description=\"Project Name:\",\n",
    "    placeholder=\"Enter your project name\",\n",
    ")\n",
    "\n",
    "# Display the input box and button\n",
    "display(project_name_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dict = create_project_folder(project_name_input.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Test-train split (incl. dropping NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "base_dir = os.path.join(root, \"data\", \"facemap_multi_video\")\n",
    "output_dir = os.path.join(project_dict, \"data\")\n",
    "\n",
    "from utils.train_test_split import split_and_organize_data\n",
    "\n",
    "# Call the function to process and combine data from multiple folders\n",
    "split_and_organize_data(base_dir, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "rotation = how much rotation (degrees) should be applied to the image\n",
    "img_height = input image height (consider changing this to automatically be derived from meta data files if expecting it not to be uniform)\n",
    "img_size = size in pixels (ViT expect 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to be applied, and input parameters to the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to associate names with each transformation\n",
    "transforms_dict = {\n",
    "    'rotate_rescale': rotate_rescale,\n",
    "    'flip_rescale': flip_rescale,\n",
    "    'pad_rescale': pad_rescale,\n",
    "    'rotate_flip_rescale': rotate_flip_rescale,\n",
    "    'blur': blur\n",
    "}\n",
    "\n",
    "\n",
    "# Import the AugmentedFaceDataset class\n",
    "from utils.Dataaugmentation import AugmentedFaceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create augmented train set\n",
    "\n",
    "# Define the paths\n",
    "source_folder = os.path.join(project_dict, \"data\")\n",
    "\n",
    "# Define paths using os.path.join for consistency\n",
    "train_csv_file = os.path.join(source_folder, \"train\", \"train_data.csv\")\n",
    "train_folder = os.path.join(source_folder, \"train\")\n",
    "train_output_dir = os.path.join(source_folder, \"train\", \"augmented_data\")\n",
    "\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=train_csv_file, root_dir=train_folder, output_dir=train_output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create augmented test set\n",
    "\n",
    "# Define the paths\n",
    "source_folder = os.path.join(project_dict, \"data\")\n",
    "\n",
    "# Define paths using os.path.join for consistency\n",
    "test_csv_file = os.path.join(source_folder, \"test\", \"test_data.csv\")\n",
    "test_folder = os.path.join(source_folder, \"test\")\n",
    "test_output_dir = os.path.join(source_folder, \"test\", \"augmented_data\")\n",
    "\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=test_csv_file, root_dir=test_folder, output_dir=test_output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to train and test augmented data\n",
    "# Train\n",
    "train_output_dir = os.path.join(source_folder, \"train\", \"augmented_data\")\n",
    "train_csv = os.path.join(train_output_dir, \"augmented_labels.csv\")\n",
    "\n",
    "# Test\n",
    "test_output_dir = os.path.join(source_folder, \"test\", \"augmented_data\")\n",
    "test_csv = os.path.join(test_output_dir, \"augmented_labels.csv\")\n",
    "\n",
    "print(train_output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Training\n",
    "\n",
    "In order to train the ViT, the following sections are run. Subprocess is used in order to run train.py from within a python script.\n",
    "\n",
    "For reference, the following arguments are to be specified for the training model.\n",
    "\n",
    "\"--name\", default=\"test\" \n",
    "--> \"Name of this run. Used for monitoring.\"\n",
    "\n",
    "\"--dataset\", default=\"facemap\" \n",
    "--> \"Which downstream task and dataset to use\"\n",
    "\n",
    "\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\", \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"], default=\"ViT-B_16\"\n",
    "-->help=\"Which variant to use.\"\n",
    "\n",
    "\"--pretrained_dir\", type=str, default=\"ViT-B_16.npz\"\n",
    "--> \"Where to search for pretrained ViT models. If not modified, will search in the directory where .ipynb project execution file is placed.\"\n",
    "\n",
    "\"--output_dir\", default=\"output\", type=str\n",
    "-->\"The output directory where checkpoints will be written.\"\n",
    "\n",
    "\"--img_size\", default=224, type=int\n",
    "--> =\"Resolution size for image\"\n",
    "\n",
    "\"--train_batch_size\", default=20, type=int\n",
    "--> \"Batch size for training.\"\n",
    "\n",
    "\"--eval_batch_size\", default=20, type=int\n",
    "h--> \"Total batch size for eval.\"\n",
    "\n",
    "\"--eval_every\", default=100, type=int,\n",
    "--> \"Run prediction on validation set every so many steps. Will always run one evaluation at the end of training.\"\n",
    "\n",
    "\"--learning_rate\", default=2e-4, type=float,\n",
    "--> \"The initial learning rate for the optimizer.\"\n",
    "\n",
    "\"--weight_decay\", default=1e-2, type=float,\n",
    "--> \"Weight deay if we apply some.\"\n",
    "\n",
    "\"--num_steps\", default=3000, type=int,\n",
    "--> \"Total number of training epochs to perform.\"\n",
    "\n",
    "\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"linear\", #changed from cosine as I believe this is what Yichen did\n",
    "--> \"How to decay the learning rate.\"\n",
    "\n",
    "\"--warmup_steps\", default=500, type=int,\n",
    "--> \"Step of training to perform learning rate warmup for.\"\n",
    "\n",
    "\"--max_grad_norm\", default=1.0, type=float,\n",
    "--> \"Max gradient norm.\"\n",
    "\n",
    "\"--local_rank\", type=int, default=-1,\n",
    "--> \"local_rank for distributed training on gpus\" - I think this might be if you have more than one GPU available, you can distribute training. Or if one GPU has more than one core\n",
    "\n",
    "'--seed', type=int, default=42,\n",
    "--> \"random seed for initialization\"\n",
    "\n",
    "'--gradient_accumulation_steps', type=int, default=1, # tried adjusting this from 1 to 25 to match Yichen\n",
    "--> \"Number of updates steps to accumulate before performing a backward/update pass.\"\n",
    "\n",
    "('--fp16', action='store_true',\n",
    "--> \"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "\n",
    "'--fp16_opt_level', type=str, default='O2',\n",
    "-->\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "\n",
    "'--loss_scale', type=float, default=0,\n",
    "-->\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train_epochs.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", model_path,\n",
    "    \"--output_dir\", project_dict,\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(2),\n",
    "    \"--num_epochs\", str(2),\n",
    "    \"--train_csv_file\", train_csv,\n",
    "    \"--train_data_dir\", train_output_dir,\n",
    "    \"--test_csv_file\", test_csv,\n",
    "    \"--test_data_dir\", test_output_dir,\n",
    "    \"--wandb_project_name\", \"facemap_project_wandb\"\n",
    "]\n",
    "\n",
    "# Use Popen to stream output\n",
    "process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "# Print the output in real-time\n",
    "for line in iter(process.stdout.readline, ''):\n",
    "    # Strip any extra newline characters for cleaner output\n",
    "    print(line.strip())\n",
    "\n",
    "# Wait for the process to finish and capture any errors\n",
    "process.wait()\n",
    "\n",
    "# Print errors, if any\n",
    "if process.returncode != 0:\n",
    "    print(f\"Errors:\\n{process.stderr.read().strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 second video for testing purpose\n",
    "\n",
    "import cv2\n",
    "\n",
    "def create_one_second_video(input_video_path, output_video_path):\n",
    "    # Open the input video file\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get the video's frames per second (fps) and size information\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Calculate the number of frames for 10 seconds\n",
    "    frames_to_extract = int(fps)*10\n",
    "    \n",
    "    # Define the codec and create a VideoWriter object to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Read and write frames to the output file\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < frames_to_extract:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"10-second video saved to {output_video_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1.avi'\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'\n",
    "create_one_second_video(input_video_path, video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the WandB API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Replace with your project path and run ID\n",
    "project_path = \"anna-stuckert-university-of-st-andrews/facemap_project\"\n",
    "\n",
    "#find the run_id by going to your run of choice, and check the URL\n",
    "# https://wandb.ai/anna-stuckert-university-of-st-andrews/facemap_project/runs/2hjtqnwr?nw=nwuserannastuckert\n",
    "# here it would be 2hjtqnwr\n",
    "run_id = \"2hjtqnwr\"\n",
    "\n",
    "# Get the specific run\n",
    "run = api.run(f\"{project_path}/{run_id}\")\n",
    "\n",
    "# Define the file path - this is the path inside the wandb repo where the model is stored. in this case, the mode(s) is stored in the output/ folder in the files folder under the run in wandb.\n",
    "file_path = \"output/facemap_with_augmentation_300epochs_checkpoint_epoch_299.pth\"\n",
    "\n",
    "# Download the file\n",
    "file = run.file(file_path)\n",
    "\n",
    "# Specify the download directory (defaults to current working directory)\n",
    "output_dir = Path(\"projects\") / \"Facemap\" / \"wandb_model\"\n",
    "\n",
    "# Download the file\n",
    "downloaded_file_path = file.download(replace=True, root=output_dir)\n",
    "\n",
    "# Print the actual download location\n",
    "print(f\"Downloaded {file.name} to {downloaded_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (embeddings): Embeddings(\n",
      "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (attn): Attention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=24, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "\n",
    "# Prepare Model\n",
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_KPs=24, zero_head=False, img_size=224, vis=True)\n",
    "# Define the path in an OS-independent way\n",
    "checkpoint_path = Path(\"projects\") / \"Facemap\" / \"wandb_model\" / \"output\" / \"facemap_with_augmentation_300epochs_checkpoint_epoch_299.pth\"\n",
    "checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))  # Load the checkpoint #delete map_location=torch.device('cpu') if run on GPU\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory: Facemap_videos/video_inference/\n",
      "Processing video: /Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Input tensor size: torch.Size([1, 197, 768])\n",
      "Video with keypoints saved to Facemap_videos/video_inference/cam1_G7c1_1_10seconds_keypoints.mp4\n",
      "Keypoints saved to Facemap_videos/video_inference/cam1_G7c1_1_10seconds_keypoints.csv\n",
      "Output video saved to: Facemap_videos/video_inference/cam1_G7c1_1_10seconds_keypoints.mp4\n",
      "Output CSV saved to: Facemap_videos/video_inference/cam1_G7c1_1_10seconds_keypoints.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import glob\n",
    "import os\n",
    "from utils.video_inference import overlay_keypoints_on_video_and_save_csv, run_inference_on_video\n",
    "# Example usage\n",
    "video_dir = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/'  # Directory containing video files\n",
    "output_dir = 'Facemap_videos/video_inference/'  # Directory to save output videos and CSVs\n",
    "\n",
    "# Assume the model is already loaded and available as `model`\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    print(f\"Creating directory: {output_dir}\")\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process each video file in the directory\n",
    "video_files = glob.glob(os.path.join(video_dir, '*.avi'))  # Adjust extension if needed\n",
    "\n",
    "for video_path in video_files:\n",
    "    base_name = os.path.basename(video_path)\n",
    "    name, _ = os.path.splitext(base_name)\n",
    "    output_video_path = os.path.join(output_dir, f'{name}_keypoints.mp4')\n",
    "    output_csv_path = os.path.join(output_dir, f'{name}_keypoints.csv')\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "\n",
    "    # Run inference on the video\n",
    "    keypoints_list = run_inference_on_video(video_path, model, device)\n",
    "\n",
    "    # Overlay keypoints and save results\n",
    "    overlay_keypoints_on_video_and_save_csv(video_path, keypoints_list, output_video_path, output_csv_path)\n",
    "\n",
    "    print(f\"Output video saved to: {output_video_path}\")\n",
    "    print(f\"Output CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model inference predictions against test_set labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "\n",
    "# Prepare Model\n",
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_KPs=24, zero_head=False, img_size=224, vis=True)\n",
    "# Define the path in an OS-independent way\n",
    "checkpoint_path = Path(\"projects\") / \"Facemap\" / \"output\" / \"facemap_without_augmentation_300epochs_testing20240907.pth\"\n",
    "checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))  # Load the checkpoint #delete map_location=torch.device('cpu') if run on GPU\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform (assuming this is required elsewhere)\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Uncomment if resizing is needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Define the data path in an OS-independent way\n",
    "data_path = Path(\"projects\") / \"Facemap\" / \"data\" / \"test\" / \"augmented_data\"\n",
    "\n",
    "# Define the image name\n",
    "imageName = 'cam1_G7c1_2_img0373_pad_rescale_augmented.jpg'\n",
    "\n",
    "# Open the image (using the / operator to concatenate the path and file name)\n",
    "im = Image.open(data_path / imageName)\n",
    "x = transform(im)\n",
    "x.size()\n",
    "\n",
    "#labels = pd.read_csv(data_path /'augmented_labels.csv')\n",
    "#labelsKepoints = labels.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:]\n",
    "\n",
    "labels = pd.read_csv(data_path /'augmented_labels.csv')\n",
    "labelsKepoints = labels.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:]\n",
    "\n",
    "#project_ouput_path = Path(\"projects\") / \"Facemap\" / \"output\"\n",
    "#preds = pd.read_csv(project_ouput_path /'predictions.csv')\n",
    "#predsKeypoints = preds.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading KP predictions form model forward pass\n",
    "# Transform the image (assuming you have a transform function defined)\n",
    "x = transform(im)\n",
    "\n",
    "# Get model output\n",
    "# Assuming the model is already defined and loaded\n",
    "out = model(x.unsqueeze(0))\n",
    "\n",
    "# Convert the model output to a format suitable for plotting\n",
    "keypoints = out[0].detach().cpu().numpy()  # Detach and move to CPU if using a GPU\n",
    "\n",
    "# Access the first row of keypoints since shape is (1, 24)\n",
    "keypoints = keypoints[0]\n",
    "\n",
    "print(keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "# Loop through the keypoints and plot them\n",
    "for i in range(0, len(keypoints), 2):\n",
    "    x_coord = keypoints[i]\n",
    "    y_coord = keypoints[i + 1]\n",
    "    plt.scatter(x_coord, y_coord, s=10, c='blue', marker='x')  # Plot each keypoint\n",
    "for i in range(0, 23, 2):\n",
    "    plt.plot(labelsKepoints[i], labelsKepoints[i+1], 'ro')\n",
    "    #plt.plot(predsKeypoints[i], predsKeypoints[i+1], 'yo', markerfacecolor='none', markersize=10) #allows us to plot x+y coordinate of each key point (i+1) and loops over the 24 keypoints, skipping every second step since we plot both x+y \n",
    " \n",
    "plt.title(\"Image with Key Points\")\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading model from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize the WandB API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Replace with your project path and run ID\n",
    "project_path = \"anna-stuckert-university-of-st-andrews/facemap_project\"\n",
    "\n",
    "#find the run_id by going to your run of choice, and check the URL\n",
    "# https://wandb.ai/anna-stuckert-university-of-st-andrews/facemap_project/runs/2hjtqnwr?nw=nwuserannastuckert\n",
    "# here it would be 2hjtqnwr\n",
    "run_id = \"2hjtqnwr\"\n",
    "\n",
    "# Get the specific run\n",
    "run = api.run(f\"{project_path}/{run_id}\")\n",
    "\n",
    "# Define the file path - this is the path inside the wandb repo where the model is stored. in this case, the mode(s) is stored in the output/ folder in the files folder under the run in wandb.\n",
    "file_path = \"output/facemap_with_augmentation_300epochs_checkpoint_epoch_299.pth\"\n",
    "\n",
    "# Download the file\n",
    "file = run.file(file_path)\n",
    "\n",
    "# Specify the download directory (defaults to current working directory)\n",
    "output_dir = Path(\"projects\") / \"Facemap\" / \"wandb_model\"\n",
    "\n",
    "# Download the file\n",
    "downloaded_file_path = file.download(replace=True, root=output_dir)\n",
    "\n",
    "# Print the actual download location\n",
    "print(f\"Downloaded {file.name} to {downloaded_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "import torch\n",
    "\n",
    "# Prepare Model\n",
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_KPs=24, zero_head=False, img_size=224, vis=True)\n",
    "# Define the path in an OS-independent way\n",
    "checkpoint_path = Path(output_dir/ file_path)\n",
    "checkpoint = torch.load(checkpoint_path,map_location=torch.device('cpu'))  # Load the checkpoint #delete map_location=torch.device('cpu') if run on GPU\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(downloaded_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize loss curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train.py producing lossCurve in steps\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        #steps = [d_lossCurve['steps'][i] for i in indices]\n",
    "        steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        if metric == 'training_loss':\n",
    "            stepsPerEpoch = 49\n",
    "            steps = np.arange(61)\n",
    "\n",
    "            loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_epochs.py, producing lossCurve in epochs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        steps = [d_lossCurve['epoch'][i] for i in indices]\n",
    "        #steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        # if metric == 'training_loss':\n",
    "        #     stepsPerEpoch = 49\n",
    "        #     steps = np.arange(61)\n",
    "\n",
    "        #     loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCK\n",
    "\n",
    "PCK measures the percentage of keypoints that are predicted correctly within a certain normalized distance from the ground truth keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Works for the original labels.csv file in the data repo\n",
    "\n",
    "def calculate_pck(labels, preds, alpha=0.2, reference_points=(4, 5)):\n",
    "    \"\"\"\n",
    "    Calculate PCK (Percentage of Correct Keypoints) for each image and identify incorrect keypoints.\n",
    "\n",
    "    Args:\n",
    "        labels (pd.DataFrame): Ground truth keypoints with image names.\n",
    "        preds (pd.DataFrame): Predicted keypoints with image names.\n",
    "        alpha (float): Threshold for PCK, typically 0.2.\n",
    "        reference_points (tuple): Indices of keypoints to use as reference for normalization.\n",
    "\n",
    "    Returns:\n",
    "        pck_results (dict): Dictionary of image_name to (PCK value, list of incorrect keypoints).\n",
    "    \"\"\"\n",
    "    pck_results = {}\n",
    "    num_keypoints = (labels.shape[1] - 1) // 2  # Assuming first column is 'image_name'\n",
    "\n",
    "    for _, row in labels.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        label_keypoints = row.values[1:].reshape(num_keypoints, 2)  # Ground truth keypoints\n",
    "        print(label_keypoints)\n",
    "        \n",
    "        pred_row = preds.loc[preds['image_names'] == image_name]\n",
    "        if pred_row.empty:\n",
    "            print(f\"No predictions found for image {image_name}\")\n",
    "            continue\n",
    "        pred_keypoints = pred_row.values[0][1:-1].reshape(num_keypoints, 2)  # Predicted keypoints\n",
    "        print(pred_keypoints)\n",
    "\n",
    "        # Calculate reference distance (e.g., shoulder distance)\n",
    "        ref_distance = np.linalg.norm(label_keypoints[reference_points[0]] - label_keypoints[reference_points[1]])\n",
    "        threshold_distance = alpha * ref_distance\n",
    "\n",
    "        correct_keypoints = 0\n",
    "        incorrect_keypoints = []  # To track which keypoints are incorrect\n",
    "\n",
    "        for j in range(num_keypoints):\n",
    "            distance = np.linalg.norm(pred_keypoints[j] - label_keypoints[j])\n",
    "            if distance < threshold_distance:\n",
    "                correct_keypoints += 1\n",
    "            else:\n",
    "                incorrect_keypoints.append(j)  # Store the index of the incorrect keypoint\n",
    "\n",
    "        pck = correct_keypoints / num_keypoints\n",
    "        pck_results[image_name] = (pck, incorrect_keypoints)  # Store PCK and incorrect keypoints\n",
    "\n",
    "    return pck_results\n",
    "\n",
    "# Load the data\n",
    "labels = pd.read_csv('augmented_data_test/augmented_labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK - automatically use the distance between eye_top and eye_bottom (which I expect to be quite stable) as reference pair (1,3).\n",
    "# KP should be within 10% of the distance between the ref pair\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKS ON THE PRODUCED LABELS\n",
    "\n",
    "import pandas as pd\n",
    "from utils.performance_metrics import calculate_pck\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.performance_metrics import calculate_pck, calculate_rmse, average_pck, average_rmse\n",
    "# Load the data\n",
    " #Usage example\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_results = calculate_rmse(labels, preds)\n",
    "\n",
    "# Print PCK and RMSE results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    rmse_value = rmse_results.get(image_name, None)\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}, RMSE: {rmse_value:.4f}\")\n",
    "\n",
    "# Calculate and print average PCK and RMSE\n",
    "avg_pck = average_pck(pck_results)\n",
    "avg_rmse = average_rmse(rmse_results)\n",
    "print(f\"Average PCK: {avg_pck:.2f}, Average RMSE: {avg_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
