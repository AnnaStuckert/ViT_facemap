{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution notebook\n",
    "This notebook serves as a walk-through of the code to execute training of the ViT keypoint tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import test_train_split\n",
    "from utils.data_utils import FaceLandmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare paths\n",
    "\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Test-train split (incl. dropping NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "csv_path = f\"{root}/data/facemap/CollectedData_AVS.csv\"\n",
    "dest_folder = f\"{root}/data/facemap/data_No_NaN\"\n",
    "source_folder = f\"{root}/data/facemap\"\n",
    "\n",
    "\n",
    "# Call the function to process data\n",
    "test_train_split(csv_path, source_folder, dest_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "rotation = how much rotation (degrees) should be applied to the image\n",
    "img_height = input image height (consider changing this to automatically be derived from meta data files if expecting it not to be uniform)\n",
    "img_size = size in pixels (ViT expect 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to be applied, and input parameters to the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a dictionary to associate names with each transformation\n",
    "transforms_dict = {\n",
    "    'rotate_rescale': rotate_rescale,\n",
    "    'flip_rescale': flip_rescale,\n",
    "    'pad_rescale': pad_rescale,\n",
    "    'rotate_flip_rescale': rotate_flip_rescale,\n",
    "    'blur': blur\n",
    "}\n",
    "\n",
    "# Import the AugmentedFaceDataset class\n",
    "from utils.Dataaugmentation import AugmentedFaceDataset\n",
    "\n",
    "# Define the paths\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n",
    "csv_file = f\"{root}/data/facemap/data_No_NaN/train/train_data.csv\"\n",
    "source_folder = f\"{root}/data/facemap/data_No_NaN/train/\"\n",
    "output_dir = f\"{root}/data/facemap/data_No_NaN/train/augmented_data\"\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=csv_file, root_dir=source_folder, output_dir=output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Training\n",
    "\n",
    "In order to train the ViT, the following sections are run. Subprocess is used in order to run train.py from within a python script.\n",
    "\n",
    "For reference, the following arguments are to be specified for the training model.\n",
    "\n",
    "\"--name\", default=\"test\" \n",
    "--> \"Name of this run. Used for monitoring.\"\n",
    "\n",
    "\"--dataset\", default=\"facemap\" \n",
    "--> \"Which downstream task and dataset to use\"\n",
    "\n",
    "\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\", \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"], default=\"ViT-B_16\"\n",
    "-->help=\"Which variant to use.\"\n",
    "\n",
    "\"--pretrained_dir\", type=str, default=\"ViT-B_16.npz\"\n",
    "--> \"Where to search for pretrained ViT models. If not modified, will search in the directory where .ipynb project execution file is placed.\"\n",
    "\n",
    "\"--output_dir\", default=\"output\", type=str\n",
    "-->\"The output directory where checkpoints will be written.\"\n",
    "\n",
    "\"--img_size\", default=224, type=int\n",
    "--> =\"Resolution size for image\"\n",
    "\n",
    "\"--train_batch_size\", default=20, type=int\n",
    "--> \"Batch size for training.\"\n",
    "\n",
    "\"--eval_batch_size\", default=20, type=int\n",
    "h--> \"Total batch size for eval.\"\n",
    "\n",
    "\"--eval_every\", default=100, type=int,\n",
    "--> \"Run prediction on validation set every so many steps. Will always run one evaluation at the end of training.\"\n",
    "\n",
    "\"--learning_rate\", default=2e-4, type=float,\n",
    "--> \"The initial learning rate for the optimizer.\"\n",
    "\n",
    "\"--weight_decay\", default=1e-2, type=float,\n",
    "--> \"Weight deay if we apply some.\"\n",
    "\n",
    "\"--num_steps\", default=3000, type=int,\n",
    "--> \"Total number of training epochs to perform.\"\n",
    "\n",
    "\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"linear\", #changed from cosine as I believe this is what Yichen did\n",
    "--> \"How to decay the learning rate.\"\n",
    "\n",
    "\"--warmup_steps\", default=500, type=int,\n",
    "--> \"Step of training to perform learning rate warmup for.\"\n",
    "\n",
    "\"--max_grad_norm\", default=1.0, type=float,\n",
    "--> \"Max gradient norm.\"\n",
    "\n",
    "\"--local_rank\", type=int, default=-1,\n",
    "--> \"local_rank for distributed training on gpus\" - I think this might be if you have more than one GPU available, you can distribute training. Or if one GPU has more than one core\n",
    "\n",
    "'--seed', type=int, default=42,\n",
    "--> \"random seed for initialization\"\n",
    "\n",
    "'--gradient_accumulation_steps', type=int, default=1, # tried adjusting this from 1 to 25 to match Yichen\n",
    "--> \"Number of updates steps to accumulate before performing a backward/update pass.\"\n",
    "\n",
    "('--fp16', action='store_true',\n",
    "--> \"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "\n",
    "'--fp16_opt_level', type=str, default='O2',\n",
    "-->\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "\n",
    "'--loss_scale', type=float, default=0,\n",
    "-->\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",  # Added missing comma here\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(5), \n",
    "    \"--num_steps\", str(5),\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train_epochs.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(2),\n",
    "    \"--num_epochs\", str(2),  # Changed from --num_steps to --num_epochs\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 second video for testing purpose\n",
    "\n",
    "import cv2\n",
    "\n",
    "def create_one_second_video(input_video_path, output_video_path):\n",
    "    # Open the input video file\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get the video's frames per second (fps) and size information\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Calculate the number of frames for 10 seconds\n",
    "    frames_to_extract = int(fps)*10\n",
    "    \n",
    "    # Define the codec and create a VideoWriter object to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Read and write frames to the output file\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < frames_to_extract:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"10-second video saved to {output_video_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1.avi'\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'\n",
    "create_one_second_video(input_video_path, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and libraries\n",
    "import torch\n",
    "from utils.video_inference import load_model, run_inference_on_video, overlay_keypoints_on_video_and_save_csv\n",
    "\n",
    "# Define paths and configuration\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'  # Path to your input video\n",
    "#checkpoint_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/model_checkpoints/test_checkpoint.pth'  # Path to your model checkpoint file\n",
    "checkpoint_path = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/output/test_checkpoint.pth\"\n",
    "output_video_path = 'output/keypoints.mp4'  # Path to save the output video\n",
    "output_csv_path = 'output/keypoints.csv'  # Path to save the keypoints CSV file\n",
    "config_name = 'ViT-B_16'  # Use the appropriate configuration name for your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Select device\n",
    "\n",
    "# Load the model\n",
    "model = load_model(checkpoint_path, config_name, device)\n",
    "\n",
    "# Run inference on the video to get predicted keypoints\n",
    "keypoints_list = run_inference_on_video(video_path, model, device)\n",
    "\n",
    "# Overlay the predicted keypoints on the video frames and save the output\n",
    "overlay_keypoints_on_video_and_save_csv(video_path, keypoints_list, output_video_path, output_csv_path)\n",
    "\n",
    "# Output paths and check files\n",
    "print(f\"Output video saved to: {output_video_path}\")\n",
    "print(f\"Output CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model inference predictions against test_set labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform (assuming this is required elsewhere)\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Uncomment if resizing is needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "#imageName = 'img6725_rotate_rescale_augmented.jpg'\n",
    "imageName = 'img8819_flip_rescale_augmented.jpg'\n",
    "im = Image.open(\"augmented_data_test/\" + imageName)\n",
    "x = transform(im)\n",
    "x.size()\n",
    "\n",
    "labels = pd.read_csv('augmented_data_test/augmented_labels.csv')\n",
    "labelsKepoints = labels.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:]\n",
    "\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "predsKeypoints = preds.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading KP predictions form model forward pass\n",
    "# Transform the image (assuming you have a transform function defined)\n",
    "x = transform(im)\n",
    "\n",
    "# Get model output\n",
    "# Assuming the model is already defined and loaded\n",
    "out = model(x.unsqueeze(0))\n",
    "\n",
    "# Convert the model output to a format suitable for plotting\n",
    "keypoints = out[0].detach().cpu().numpy()  # Detach and move to CPU if using a GPU\n",
    "\n",
    "# Access the first row of keypoints since shape is (1, 24)\n",
    "keypoints = keypoints[0]\n",
    "\n",
    "print(keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "# Loop through the keypoints and plot them\n",
    "for i in range(0, len(keypoints), 2):\n",
    "    x_coord = keypoints[i]\n",
    "    y_coord = keypoints[i + 1]\n",
    "    plt.scatter(x_coord, y_coord, s=10, c='blue', marker='x')  # Plot each keypoint\n",
    "for i in range(0, 23, 2):\n",
    "    plt.plot(labelsKepoints[i], labelsKepoints[i+1], 'ro')\n",
    "    plt.plot(predsKeypoints[i], predsKeypoints[i+1], 'yo', markerfacecolor='none', markersize=10) #allows us to plot x+y coordinate of each key point (i+1) and loops over the 24 keypoints, skipping every second step since we plot both x+y \n",
    " \n",
    "plt.title(\"Image with Key Points\")\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize loss curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train.py producing lossCurve in steps\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        #steps = [d_lossCurve['steps'][i] for i in indices]\n",
    "        steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        if metric == 'training_loss':\n",
    "            stepsPerEpoch = 49\n",
    "            steps = np.arange(61)\n",
    "\n",
    "            loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_epochs.py, producing lossCurve in epochs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        steps = [d_lossCurve['epoch'][i] for i in indices]\n",
    "        #steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        # if metric == 'training_loss':\n",
    "        #     stepsPerEpoch = 49\n",
    "        #     steps = np.arange(61)\n",
    "\n",
    "        #     loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCK\n",
    "\n",
    "PCK measures the percentage of keypoints that are predicted correctly within a certain normalized distance from the ground truth keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Works for the original labels.csv file in the data repo\n",
    "\n",
    "def calculate_pck(labels, preds, alpha=0.2, reference_points=(4, 5)):\n",
    "    \"\"\"\n",
    "    Calculate PCK (Percentage of Correct Keypoints) for each image and identify incorrect keypoints.\n",
    "\n",
    "    Args:\n",
    "        labels (pd.DataFrame): Ground truth keypoints with image names.\n",
    "        preds (pd.DataFrame): Predicted keypoints with image names.\n",
    "        alpha (float): Threshold for PCK, typically 0.2.\n",
    "        reference_points (tuple): Indices of keypoints to use as reference for normalization.\n",
    "\n",
    "    Returns:\n",
    "        pck_results (dict): Dictionary of image_name to (PCK value, list of incorrect keypoints).\n",
    "    \"\"\"\n",
    "    pck_results = {}\n",
    "    num_keypoints = (labels.shape[1] - 1) // 2  # Assuming first column is 'image_name'\n",
    "\n",
    "    for _, row in labels.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        label_keypoints = row.values[1:].reshape(num_keypoints, 2)  # Ground truth keypoints\n",
    "        print(label_keypoints)\n",
    "        \n",
    "        pred_row = preds.loc[preds['image_names'] == image_name]\n",
    "        if pred_row.empty:\n",
    "            print(f\"No predictions found for image {image_name}\")\n",
    "            continue\n",
    "        pred_keypoints = pred_row.values[0][1:-1].reshape(num_keypoints, 2)  # Predicted keypoints\n",
    "        print(pred_keypoints)\n",
    "\n",
    "        # Calculate reference distance (e.g., shoulder distance)\n",
    "        ref_distance = np.linalg.norm(label_keypoints[reference_points[0]] - label_keypoints[reference_points[1]])\n",
    "        threshold_distance = alpha * ref_distance\n",
    "\n",
    "        correct_keypoints = 0\n",
    "        incorrect_keypoints = []  # To track which keypoints are incorrect\n",
    "\n",
    "        for j in range(num_keypoints):\n",
    "            distance = np.linalg.norm(pred_keypoints[j] - label_keypoints[j])\n",
    "            if distance < threshold_distance:\n",
    "                correct_keypoints += 1\n",
    "            else:\n",
    "                incorrect_keypoints.append(j)  # Store the index of the incorrect keypoint\n",
    "\n",
    "        pck = correct_keypoints / num_keypoints\n",
    "        pck_results[image_name] = (pck, incorrect_keypoints)  # Store PCK and incorrect keypoints\n",
    "\n",
    "    return pck_results\n",
    "\n",
    "# Load the data\n",
    "labels = pd.read_csv('augmented_data_test/augmented_labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK - automatically use the distance between eye_top and eye_bottom (which I expect to be quite stable) as reference pair (1,3).\n",
    "# KP should be within 10% of the distance between the ref pair\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: img8504_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 8, 9]\n",
      "Image: img8504_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 8, 9, 10]\n",
      "Image: img8504_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8504_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8504_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8687_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 7, 9, 10]\n",
      "Image: img8687_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 7, 9, 10, 11]\n",
      "Image: img8687_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img8687_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img8687_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img8819_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img8819_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 11]\n",
      "Image: img8819_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8819_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img8819_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8967_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img8967_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img8967_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img8967_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img8967_blur_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img9298_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img9298_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [3, 4, 5, 7, 9]\n",
      "Image: img9298_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img9298_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [3, 5, 7, 9]\n",
      "Image: img9298_blur_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img9797_rotate_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 7, 8, 9, 10]\n",
      "Image: img9797_flip_rescale_augmented.jpg, PCK: 0.42, Incorrect Keypoints: [3, 4, 5, 7, 8, 9, 10]\n",
      "Image: img9797_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img9797_rotate_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [3, 5, 6, 7, 9]\n",
      "Image: img9797_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [5, 6, 7, 9, 10]\n",
      "Image: img9811_rotate_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 10]\n",
      "Image: img9811_flip_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 7, 9, 10, 11]\n",
      "Image: img9811_pad_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [5, 6, 7, 9, 10]\n",
      "Image: img9811_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img9811_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [5, 6, 7, 9, 10]\n",
      "Image: img9924_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 9, 10]\n",
      "Image: img9924_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 6, 9]\n",
      "Image: img9924_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 10]\n",
      "Image: img9924_rotate_flip_rescale_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [9]\n",
      "Image: img9924_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 6, 7]\n",
      "Image: img7134_rotate_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7134_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7134_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7134_rotate_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7134_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7165_rotate_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [5, 7]\n",
      "Image: img7165_flip_rescale_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [7]\n",
      "Image: img7165_pad_rescale_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [7]\n",
      "Image: img7165_rotate_flip_rescale_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [7]\n",
      "Image: img7165_blur_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [7]\n",
      "Image: img7202_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img7202_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img7202_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7202_rotate_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7202_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7211_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 7, 9, 10]\n",
      "Image: img7211_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 9, 10]\n",
      "Image: img7211_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 10]\n",
      "Image: img7211_rotate_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img7211_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 10]\n",
      "Image: img7662_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7662_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [9, 10, 11]\n",
      "Image: img7662_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7662_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7662_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 7, 9, 10, 11]\n",
      "Image: img7673_rotate_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 6, 7, 8, 9]\n",
      "Image: img7673_flip_rescale_augmented.jpg, PCK: 0.42, Incorrect Keypoints: [4, 5, 6, 7, 8, 9, 11]\n",
      "Image: img7673_pad_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 10]\n",
      "Image: img7673_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img7673_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 10]\n",
      "Image: img7965_rotate_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 8, 9]\n",
      "Image: img7965_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 8, 9]\n",
      "Image: img7965_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [6, 7, 9]\n",
      "Image: img7965_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7965_blur_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img8048_rotate_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 7, 8, 9, 10]\n",
      "Image: img8048_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 8, 9]\n",
      "Image: img8048_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8048_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8048_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8261_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img8261_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 8, 9]\n",
      "Image: img8261_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img8261_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img8261_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img8371_rotate_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 6, 7, 8, 9]\n",
      "Image: img8371_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img8371_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 10]\n",
      "Image: img8371_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img8371_blur_augmented.jpg, PCK: 0.92, Incorrect Keypoints: [7]\n",
      "Image: img6700_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 7, 8, 9]\n",
      "Image: img6700_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 7, 8, 9]\n",
      "Image: img6700_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 6, 7]\n",
      "Image: img6700_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img6700_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img6725_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img6725_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 8, 9, 10]\n",
      "Image: img6725_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 10]\n",
      "Image: img6725_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 9]\n",
      "Image: img6725_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [4, 5, 10]\n",
      "Image: img6753_rotate_rescale_augmented.jpg, PCK: 0.33, Incorrect Keypoints: [4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Image: img6753_flip_rescale_augmented.jpg, PCK: 0.33, Incorrect Keypoints: [4, 5, 6, 7, 8, 9, 10, 11]\n",
      "Image: img6753_pad_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 11]\n",
      "Image: img6753_rotate_flip_rescale_augmented.jpg, PCK: 0.42, Incorrect Keypoints: [4, 5, 6, 7, 9, 10, 11]\n",
      "Image: img6753_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 11]\n",
      "Image: img6760_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 8, 9]\n",
      "Image: img6760_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 8, 9]\n",
      "Image: img6760_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img6760_rotate_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img6760_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 9]\n",
      "Image: img6888_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6888_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6888_pad_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 10]\n",
      "Image: img6888_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6888_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 7, 9, 10]\n",
      "Image: img6985_rotate_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6985_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6985_pad_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img6985_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [4, 5, 7, 9]\n",
      "Image: img6985_blur_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [5, 6, 7, 9]\n",
      "Image: img7006_rotate_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [5, 7]\n",
      "Image: img7006_flip_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [5, 9]\n",
      "Image: img7006_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [5, 7]\n",
      "Image: img7006_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7006_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [5, 7]\n",
      "Image: img7011_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 10]\n",
      "Image: img7011_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7011_pad_rescale_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 10]\n",
      "Image: img7011_rotate_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [7, 10, 11]\n",
      "Image: img7011_blur_augmented.jpg, PCK: 0.83, Incorrect Keypoints: [7, 10]\n",
      "Image: img7019_rotate_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7019_flip_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7019_pad_rescale_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7019_rotate_flip_rescale_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [5, 6, 7, 9, 11]\n",
      "Image: img7019_blur_augmented.jpg, PCK: 0.75, Incorrect Keypoints: [5, 7, 9]\n",
      "Image: img7060_rotate_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [5, 7, 8, 9, 10, 11]\n",
      "Image: img7060_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7060_pad_rescale_augmented.jpg, PCK: 0.50, Incorrect Keypoints: [4, 5, 6, 7, 9, 10]\n",
      "Image: img7060_rotate_flip_rescale_augmented.jpg, PCK: 0.67, Incorrect Keypoints: [7, 9, 10, 11]\n",
      "Image: img7060_blur_augmented.jpg, PCK: 0.58, Incorrect Keypoints: [4, 5, 6, 7, 9]\n"
     ]
    }
   ],
   "source": [
    "#WORKS ON THE PRODUCED LABELS\n",
    "\n",
    "import pandas as pd\n",
    "from utils.performance_metrics import calculate_pck\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'calculate_rmse' from 'utils.performance_metrics' (/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/utils/performance_metrics.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mperformance_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_pck, calculate_rmse, average_pck, average_rmse\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m  \u001b[38;5;66;03m#Usage example\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'calculate_rmse' from 'utils.performance_metrics' (/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/utils/performance_metrics.py)"
     ]
    }
   ],
   "source": [
    "from utils.performance_metrics import calculate_pck, calculate_rmse, average_pck, average_rmse\n",
    "# Load the data\n",
    " #Usage example\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_results = calculate_rmse(labels, preds)\n",
    "\n",
    "# Print PCK and RMSE results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    rmse_value = rmse_results.get(image_name, None)\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}, RMSE: {rmse_value:.4f}\")\n",
    "\n",
    "# Calculate and print average PCK and RMSE\n",
    "avg_pck = average_pck(pck_results)\n",
    "avg_rmse = average_rmse(rmse_results)\n",
    "print(f\"Average PCK: {avg_pck:.2f}, Average RMSE: {avg_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
