{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution notebook\n",
    "This notebook serves as a walk-through of the code to execute training of the ViT keypoint tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import test_train_split\n",
    "from utils.data_utils import FaceLandmarksDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare paths\n",
    "\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Test-train split (incl. dropping NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "csv_path = f\"{root}/data/facemap/CollectedData_AVS.csv\"\n",
    "dest_folder = f\"{root}/data/facemap/data_No_NaN\"\n",
    "source_folder = f\"{root}/data/facemap\"\n",
    "\n",
    "\n",
    "# Call the function to process data\n",
    "test_train_split(csv_path, source_folder, dest_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "rotation = how much rotation (degrees) should be applied to the image\n",
    "img_height = input image height (consider changing this to automatically be derived from meta data files if expecting it not to be uniform)\n",
    "img_size = size in pixels (ViT expect 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Dataaugmentation import AugmentedFaceDataset\n",
    "\n",
    "transforms_list = [\n",
    "    \"rotate_rescale\",\n",
    "    \"flip_rescale\",\n",
    "    \"pad_rescale\",\n",
    "    \"rotate_flip_rescale\",\n",
    "    \"blur\",]\n",
    "transform_params = {\n",
    "    \"rotate_rescale\": {\"rotation\": 30, \"img_height\": 256, \"img_size\": (224, 224)},\n",
    "    \"flip_rescale\": {\"img_height\": 256, \"img_size\": (224, 224)},\n",
    "    \"pad_rescale\": {\"img_height\": 300, \"img_size\": (224, 224)},\n",
    "    \"rotate_flip_rescale\": {\n",
    "        \"rotation\": 30,\n",
    "        \"img_height\": 256,\n",
    "        \"img_size\": (224, 224),\n",
    "    },\n",
    "    \"blur\": {\"img_height\": 256, \"img_size\": (224, 224)},\n",
    "}\n",
    "\n",
    "\n",
    "    # Apply transformations and save augmented data\n",
    "face_dataset.apply_transforms_and_save()\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# transforms_list = [rotate_rescale, flip_rescale, pad_rescale, rotate_flip_rescale, blur]\n",
    "# face_dataset = AugmentedFaceDataset(\n",
    "#     csv_file=\"f{dest_folder}/train/train_data.csv\",\n",
    "#     root_dir=\"f{dest_folder}/train/\",\n",
    "#     output_dir=\"f{dest_folder}/train/augmented_data/\",\n",
    "# )\n",
    "# # face_dataset = AugmentedFaceDataset(csv_file='data/facemap/LabeledData/Test/CollectedDataTest.csv', root_dir='data/facemap/LabeledData/Test/', output_dir='augmented_data_test/')\n",
    "# face_dataset.apply_transforms_and_save(transforms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data saved to: /Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/data/facemap/data_No_NaN/train/augmented_data/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary components from the Dataaugmentation module\n",
    "from utils.Dataaugmentation import AugmentedFaceDataset, rotate_rescale, flip_rescale, pad_rescale, rotate_flip_rescale, blur\n",
    "\n",
    "# Define the transformation list and parameters\n",
    "transforms_list = [\n",
    "    \"rotate_rescale\",\n",
    "    \"flip_rescale\",\n",
    "    \"pad_rescale\",\n",
    "    \"rotate_flip_rescale\",\n",
    "    \"blur\",\n",
    "]\n",
    "transform_params = {\n",
    "    \"rotation\": 10,\n",
    "    \"img_height\": 256,\n",
    "    \"img_size\": (224, 224),\n",
    "}\n",
    "\n",
    "# Set up dataset\n",
    "face_dataset = AugmentedFaceDataset(\n",
    "    csv_file=\"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/data/facemap/data_No_NaN/train/train_data.csv\",  # Adjust path as necessary\n",
    "    root_dir=\"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/data/facemap/data_No_NaN/train\",  # Adjust path as necessary\n",
    "    output_dir=\"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/data/facemap/data_No_NaN/train/augmented_data/\",  # Adjust path as necessary\n",
    "    output_size=(224, 224),\n",
    "    transform_list=transforms_list,\n",
    "    transform_params=transform_params,\n",
    ")\n",
    "\n",
    "# Apply transformations and save augmented data\n",
    "face_dataset.apply_transforms_and_save()\n",
    "\n",
    "# Print output directory to confirm\n",
    "print(\"Augmented data saved to:\", face_dataset.output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Training\n",
    "\n",
    "In order to train the ViT, the following sections are run. Subprocess is used in order to run train.py from within a python script.\n",
    "\n",
    "For reference, the following arguments are to be specified for the training model.\n",
    "\n",
    "\"--name\", default=\"test\" \n",
    "--> \"Name of this run. Used for monitoring.\"\n",
    "\n",
    "\"--dataset\", default=\"facemap\" \n",
    "--> \"Which downstream task and dataset to use\"\n",
    "\n",
    "\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\", \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"], default=\"ViT-B_16\"\n",
    "-->help=\"Which variant to use.\"\n",
    "\n",
    "\"--pretrained_dir\", type=str, default=\"ViT-B_16.npz\"\n",
    "--> \"Where to search for pretrained ViT models. If not modified, will search in the directory where .ipynb project execution file is placed.\"\n",
    "\n",
    "\"--output_dir\", default=\"output\", type=str\n",
    "-->\"The output directory where checkpoints will be written.\"\n",
    "\n",
    "\"--img_size\", default=224, type=int\n",
    "--> =\"Resolution size for image\"\n",
    "\n",
    "\"--train_batch_size\", default=20, type=int\n",
    "--> \"Batch size for training.\"\n",
    "\n",
    "\"--eval_batch_size\", default=20, type=int\n",
    "h--> \"Total batch size for eval.\"\n",
    "\n",
    "\"--eval_every\", default=100, type=int,\n",
    "--> \"Run prediction on validation set every so many steps. Will always run one evaluation at the end of training.\"\n",
    "\n",
    "\"--learning_rate\", default=2e-4, type=float,\n",
    "--> \"The initial learning rate for the optimizer.\"\n",
    "\n",
    "\"--weight_decay\", default=1e-2, type=float,\n",
    "--> \"Weight deay if we apply some.\"\n",
    "\n",
    "\"--num_steps\", default=3000, type=int,\n",
    "--> \"Total number of training epochs to perform.\"\n",
    "\n",
    "\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"linear\", #changed from cosine as I believe this is what Yichen did\n",
    "--> \"How to decay the learning rate.\"\n",
    "\n",
    "\"--warmup_steps\", default=500, type=int,\n",
    "--> \"Step of training to perform learning rate warmup for.\"\n",
    "\n",
    "\"--max_grad_norm\", default=1.0, type=float,\n",
    "--> \"Max gradient norm.\"\n",
    "\n",
    "\"--local_rank\", type=int, default=-1,\n",
    "--> \"local_rank for distributed training on gpus\" - I think this might be if you have more than one GPU available, you can distribute training. Or if one GPU has more than one core\n",
    "\n",
    "'--seed', type=int, default=42,\n",
    "--> \"random seed for initialization\"\n",
    "\n",
    "'--gradient_accumulation_steps', type=int, default=1, # tried adjusting this from 1 to 25 to match Yichen\n",
    "--> \"Number of updates steps to accumulate before performing a backward/update pass.\"\n",
    "\n",
    "('--fp16', action='store_true',\n",
    "--> \"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "\n",
    "'--fp16_opt_level', type=str, default='O2',\n",
    "-->\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "\n",
    "'--loss_scale', type=float, default=0,\n",
    "-->\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",  # Added missing comma here\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(5), \n",
    "    \"--num_steps\", str(5),\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train_epochs.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(2),\n",
    "    \"--num_epochs\", str(2),  # Changed from --num_steps to --num_epochs\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 second video for testing purpose\n",
    "\n",
    "import cv2\n",
    "\n",
    "def create_one_second_video(input_video_path, output_video_path):\n",
    "    # Open the input video file\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get the video's frames per second (fps) and size information\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Calculate the number of frames for 10 seconds\n",
    "    frames_to_extract = int(fps)*10\n",
    "    \n",
    "    # Define the codec and create a VideoWriter object to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Read and write frames to the output file\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < frames_to_extract:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"10-second video saved to {output_video_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1.avi'\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'\n",
    "create_one_second_video(input_video_path, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and libraries\n",
    "import torch\n",
    "from utils.video_inference import load_model, run_inference_on_video, overlay_keypoints_on_video_and_save_csv\n",
    "\n",
    "# Define paths and configuration\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'  # Path to your input video\n",
    "checkpoint_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/model_checkpoints/test_checkpoint.pth'  # Path to your model checkpoint file\n",
    "output_video_path = 'output/keypoints.mp4'  # Path to save the output video\n",
    "output_csv_path = 'output/keypoints.csv'  # Path to save the keypoints CSV file\n",
    "config_name = 'ViT-B_16'  # Use the appropriate configuration name for your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Select device\n",
    "\n",
    "# Load the model\n",
    "model = load_model(checkpoint_path, config_name, device)\n",
    "\n",
    "# Run inference on the video to get predicted keypoints\n",
    "keypoints_list = run_inference_on_video(video_path, model, device)\n",
    "\n",
    "# Overlay the predicted keypoints on the video frames and save the output\n",
    "overlay_keypoints_on_video_and_save_csv(video_path, keypoints_list, output_video_path, output_csv_path)\n",
    "\n",
    "# Output paths and check files\n",
    "print(f\"Output video saved to: {output_video_path}\")\n",
    "print(f\"Output CSV saved to: {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
