{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution notebook\n",
    "This notebook serves as a walk-through of the code to execute training of the ViT keypoint tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train_test_split import test_train_split\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare paths\n",
    "\n",
    "root = r\"C:\\Users\\avs20\\Documents\\GitHub\\ViT_facemap\\ViT-pytorch\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Test-train split (incl. dropping NAs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid image name: nan\n",
      "Invalid image name: nan\n",
      "Invalid image name: nan\n",
      "Invalid image name: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(           scorer   Unnamed: 1   Unnamed: 2                AVS  \\\n",
       " 3    labeled-data  cam1_G7c1_1  img0068.png  589.0232336327339   \n",
       " 43   labeled-data  cam1_G7c1_1  img2997.png  590.5093017792906   \n",
       " 48   labeled-data  cam1_G7c1_1  img3575.png  589.0232336327339   \n",
       " 68   labeled-data  cam1_G7c1_1  img5379.png  581.5928928999506   \n",
       " 79   labeled-data  cam1_G7c1_1  img6392.png  586.0510973396206   \n",
       " 89   labeled-data  cam1_G7c1_1  img7197.png  583.0789610465073   \n",
       " 105  labeled-data  cam1_G7c1_1  img8629.png  587.5371654861773   \n",
       " \n",
       "                  AVS.1               AVS.2               AVS.3  \\\n",
       " 3    67.63932048311811  502.83128113244777  106.27709229359114   \n",
       " 43   67.63932048311811  507.28948557211777  106.27709229359114   \n",
       " 48   64.66718419000479  507.28948557211777  101.81888785392117   \n",
       " 68   64.66718419000479   498.3730766927779  122.62384190571437   \n",
       " 79   67.63932048311811  507.28948557211777  109.24922858670446   \n",
       " 89   67.63932048311811   504.3173492790045  104.79102414703449   \n",
       " 105  64.66718419000479   508.7755537186745  112.22136487981778   \n",
       " \n",
       "                  AVS.4               AVS.5              AVS.6  ...  \\\n",
       " 3    562.2740069947141   173.1501588886407  636.5774143225469  ...   \n",
       " 43   566.7322114343841  171.66409074208406  633.6052780294336  ...   \n",
       " 48   568.2182795809407   174.6362270351974  636.5774143225469  ...   \n",
       " 68   574.1625521671674   174.6362270351974  638.0634824691036  ...   \n",
       " 79   574.1625521671674   171.6640907420841  635.0913461759903  ...   \n",
       " 89   566.7322114343841  167.20588630241411  636.5774143225469  ...   \n",
       " 105   571.190415874054  171.66409074208406  633.6052780294336  ...   \n",
       " \n",
       "                  AVS.14              AVS.15              AVS.16  \\\n",
       " 3    369.08514794234867  242.99536177680352   355.7105346233388   \n",
       " 43    370.5712160889053  242.99536177680346   361.6548072095654   \n",
       " 48    397.3204427269252  238.53715733713358  348.28019389055544   \n",
       " 68    378.0015568216886  237.05108919057687  339.36378501121555   \n",
       " 79    366.1130116492354   250.4257025095868  349.76626203711214   \n",
       " 89    345.3080575974422  190.98297664732053  334.90558057154556   \n",
       " 105  297.75387690762915  232.59288475090688   325.9891716922056   \n",
       " \n",
       "                  AVS.17              AVS.18              AVS.19  \\\n",
       " 3     295.0077469062865  300.72601320074244   391.6021764324692   \n",
       " 43   196.92724923354717   311.1284902266391  385.65790384624256   \n",
       " 48   185.03870406109388  305.18421764041244   387.1439719927992   \n",
       " 68    286.0913380269466  305.18421764041244   391.6021764324692   \n",
       " 79   189.49690850076388   337.8777168646589  284.60526988038987   \n",
       " 89    280.1470654407199  315.58669466630903   390.1161082859125   \n",
       " 105  195.44118108699047   311.1284902266391  393.08824457902585   \n",
       " \n",
       "                  AVS.20              AVS.21 AVS.22 AVS.23  \n",
       " 3     348.2801938905555  428.75388009638567    NaN    NaN  \n",
       " 43   357.19660276989543   421.3235393636023    NaN    NaN  \n",
       " 48    345.3080575974422  413.89319863081903    NaN    NaN  \n",
       " 68    343.8219894508855    418.351403070489    NaN    NaN  \n",
       " 79                  NaN                 NaN    NaN    NaN  \n",
       " 89    343.8219894508855  413.89319863081903    NaN    NaN  \n",
       " 105  342.33592130432885   419.8374712170457    NaN    NaN  \n",
       " \n",
       " [7 rows x 27 columns],\n",
       " 'C:\\\\Users\\\\avs20\\\\Documents\\\\GitHub\\\\ViT_facemap\\\\ViT-pytorch\\\\data\\\\facemap\\\\data_No_NaN\\\\train\\\\train_data.csv',\n",
       " 'C:\\\\Users\\\\avs20\\\\Documents\\\\GitHub\\\\ViT_facemap\\\\ViT-pytorch\\\\data\\\\facemap\\\\data_No_NaN\\\\test\\\\test_data.csv')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define paths\n",
    "csv_path = os.path.join(root, \"data\", \"facemap\", \"CollectedData_AVS.csv\")\n",
    "dest_folder = os.path.join(root, \"data\", \"facemap\", \"NaN_removed\")\n",
    "source_folder = os.path.join(root, \"data\", \"facemap\")\n",
    "\n",
    "\n",
    "# Call the function to process data\n",
    "test_train_split(csv_path, source_folder, dest_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "rotation = how much rotation (degrees) should be applied to the image\n",
    "img_height = input image height (consider changing this to automatically be derived from meta data files if expecting it not to be uniform)\n",
    "img_size = size in pixels (ViT expect 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformations to be applied, and input parameters to the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "from torchvision import transforms, utils\n",
    "import importlib\n",
    "from utils.Dataaugmentation import Rotate, ZeroPadHeight, Rescale, HorizontalFlip, GaussianBlur\n",
    "\n",
    "#from utils import Dataaugmentation\n",
    "#importlib.reload(Dataaugmentation)\n",
    "\n",
    "# Set the parameters for image augmentation\n",
    "rotation = 10  # Degrees to rotate image\n",
    "img_width = 846  # Width of the input image to pad the height to match\n",
    "final_im_size = 224  # Final image size (224x224 pixels)\n",
    "\n",
    "# Define transformations using transforms.Compose\n",
    "rotate_rescale = transforms.Compose([\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "pad_rescale = transforms.Compose([\n",
    "    ZeroPadHeight(img_width),  # Use img_width instead of hardcoded value\n",
    "    Rescale(final_im_size),\n",
    "])\n",
    "\n",
    "rotate_flip_rescale = transforms.Compose([\n",
    "    HorizontalFlip(),\n",
    "    Rotate(rotation),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])\n",
    "\n",
    "blur = transforms.Compose([\n",
    "    GaussianBlur(),\n",
    "    ZeroPadHeight(img_width),\n",
    "    Rescale(final_im_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define a dictionary to associate names with each transformation\n",
    "transforms_dict = {\n",
    "    'rotate_rescale': rotate_rescale,\n",
    "    'flip_rescale': flip_rescale,\n",
    "    'pad_rescale': pad_rescale,\n",
    "    'rotate_flip_rescale': rotate_flip_rescale,\n",
    "    'blur': blur\n",
    "}\n",
    "\n",
    "# Import the AugmentedFaceDataset class\n",
    "from utils.Dataaugmentation import AugmentedFaceDataset\n",
    "\n",
    "# Define the paths\n",
    "root = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch\"\n",
    "csv_file = f\"{root}/data/facemap/data_No_NaN/train/train_data.csv\"\n",
    "source_folder = f\"{root}/data/facemap/data_No_NaN/train/\"\n",
    "output_dir = f\"{root}/data/facemap/data_No_NaN/train/augmented_data\"\n",
    "\n",
    "# Initialize the dataset with defined transformations\n",
    "face_dataset = AugmentedFaceDataset(csv_file=csv_file, root_dir=source_folder, output_dir=output_dir)\n",
    "\n",
    "# Apply the transformations and save\n",
    "face_dataset.apply_transforms_and_save(transforms_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Training\n",
    "\n",
    "In order to train the ViT, the following sections are run. Subprocess is used in order to run train.py from within a python script.\n",
    "\n",
    "For reference, the following arguments are to be specified for the training model.\n",
    "\n",
    "\"--name\", default=\"test\" \n",
    "--> \"Name of this run. Used for monitoring.\"\n",
    "\n",
    "\"--dataset\", default=\"facemap\" \n",
    "--> \"Which downstream task and dataset to use\"\n",
    "\n",
    "\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\", \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"], default=\"ViT-B_16\"\n",
    "-->help=\"Which variant to use.\"\n",
    "\n",
    "\"--pretrained_dir\", type=str, default=\"ViT-B_16.npz\"\n",
    "--> \"Where to search for pretrained ViT models. If not modified, will search in the directory where .ipynb project execution file is placed.\"\n",
    "\n",
    "\"--output_dir\", default=\"output\", type=str\n",
    "-->\"The output directory where checkpoints will be written.\"\n",
    "\n",
    "\"--img_size\", default=224, type=int\n",
    "--> =\"Resolution size for image\"\n",
    "\n",
    "\"--train_batch_size\", default=20, type=int\n",
    "--> \"Batch size for training.\"\n",
    "\n",
    "\"--eval_batch_size\", default=20, type=int\n",
    "h--> \"Total batch size for eval.\"\n",
    "\n",
    "\"--eval_every\", default=100, type=int,\n",
    "--> \"Run prediction on validation set every so many steps. Will always run one evaluation at the end of training.\"\n",
    "\n",
    "\"--learning_rate\", default=2e-4, type=float,\n",
    "--> \"The initial learning rate for the optimizer.\"\n",
    "\n",
    "\"--weight_decay\", default=1e-2, type=float,\n",
    "--> \"Weight deay if we apply some.\"\n",
    "\n",
    "\"--num_steps\", default=3000, type=int,\n",
    "--> \"Total number of training epochs to perform.\"\n",
    "\n",
    "\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"linear\", #changed from cosine as I believe this is what Yichen did\n",
    "--> \"How to decay the learning rate.\"\n",
    "\n",
    "\"--warmup_steps\", default=500, type=int,\n",
    "--> \"Step of training to perform learning rate warmup for.\"\n",
    "\n",
    "\"--max_grad_norm\", default=1.0, type=float,\n",
    "--> \"Max gradient norm.\"\n",
    "\n",
    "\"--local_rank\", type=int, default=-1,\n",
    "--> \"local_rank for distributed training on gpus\" - I think this might be if you have more than one GPU available, you can distribute training. Or if one GPU has more than one core\n",
    "\n",
    "'--seed', type=int, default=42,\n",
    "--> \"random seed for initialization\"\n",
    "\n",
    "'--gradient_accumulation_steps', type=int, default=1, # tried adjusting this from 1 to 25 to match Yichen\n",
    "--> \"Number of updates steps to accumulate before performing a backward/update pass.\"\n",
    "\n",
    "('--fp16', action='store_true',\n",
    "--> \"Whether to use 16-bit float precision instead of 32-bit\")\n",
    "\n",
    "'--fp16_opt_level', type=str, default='O2',\n",
    "-->\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                             \"See details at https://nvidia.github.io/apex/amp.html\")\n",
    "\n",
    "'--loss_scale', type=float, default=0,\n",
    "-->\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",  # Added missing comma here\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(5), \n",
    "    \"--num_steps\", str(5),\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run the script with arguments\n",
    "command = [\n",
    "    \"python\", \"train_epochs.py\",\n",
    "    \"--name\", \"experiment_20240825\",\n",
    "    \"--dataset\", \"facemap\",\n",
    "    \"--model_type\", \"ViT-B_16\",\n",
    "    \"--pretrained_dir\", \"ViT-B_16.npz\",\n",
    "    \"--output_dir\", \"model_checkpoints\",\n",
    "    \"--train_batch_size\", str(20),\n",
    "    \"--eval_batch_size\", str(20),\n",
    "    \"--eval_every\", str(2),\n",
    "    \"--num_epochs\", str(2),  # Changed from --num_steps to --num_epochs\n",
    "]\n",
    "\n",
    "# Run the script\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output and errors (if any)\n",
    "print(\"Output:\", result.stdout)\n",
    "print(\"Errors:\", result.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 second video for testing purpose\n",
    "\n",
    "import cv2\n",
    "\n",
    "def create_one_second_video(input_video_path, output_video_path):\n",
    "    # Open the input video file\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    \n",
    "    # Check if the video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "    \n",
    "    # Get the video's frames per second (fps) and size information\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Calculate the number of frames for 10 seconds\n",
    "    frames_to_extract = int(fps)*10\n",
    "    \n",
    "    # Define the codec and create a VideoWriter object to save the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Read and write frames to the output file\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < frames_to_extract:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"10-second video saved to {output_video_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1.avi'\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'\n",
    "create_one_second_video(input_video_path, video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions and libraries\n",
    "import torch\n",
    "from utils.video_inference import load_model, run_inference_on_video, overlay_keypoints_on_video_and_save_csv\n",
    "\n",
    "# Define paths and configuration\n",
    "video_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/Facemap_videos/cam1_G7c1_1_10seconds.avi'  # Path to your input video\n",
    "#checkpoint_path = '/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/model_checkpoints/test_checkpoint.pth'  # Path to your model checkpoint file\n",
    "checkpoint_path = \"/Users/annastuckert/Documents/GitHub/ViT_facemap/ViT-pytorch/output/test_checkpoint.pth\"\n",
    "output_video_path = 'output/keypoints.mp4'  # Path to save the output video\n",
    "output_csv_path = 'output/keypoints.csv'  # Path to save the keypoints CSV file\n",
    "config_name = 'ViT-B_16'  # Use the appropriate configuration name for your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Select device\n",
    "\n",
    "# Load the model\n",
    "model = load_model(checkpoint_path, config_name, device)\n",
    "\n",
    "# Run inference on the video to get predicted keypoints\n",
    "keypoints_list = run_inference_on_video(video_path, model, device)\n",
    "\n",
    "# Overlay the predicted keypoints on the video frames and save the output\n",
    "overlay_keypoints_on_video_and_save_csv(video_path, keypoints_list, output_video_path, output_csv_path)\n",
    "\n",
    "# Output paths and check files\n",
    "print(f\"Output video saved to: {output_video_path}\")\n",
    "print(f\"Output CSV saved to: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check model inference predictions against test_set labels and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transform (assuming this is required elsewhere)\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),  # Uncomment if resizing is needed\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "#imageName = 'img6725_rotate_rescale_augmented.jpg'\n",
    "imageName = 'img8819_flip_rescale_augmented.jpg'\n",
    "im = Image.open(\"augmented_data_test/\" + imageName)\n",
    "x = transform(im)\n",
    "x.size()\n",
    "\n",
    "labels = pd.read_csv('augmented_data_test/augmented_labels.csv')\n",
    "labelsKepoints = labels.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:]\n",
    "\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "predsKeypoints = preds.loc[labels['image_name'] == imageName].values.flatten().tolist()[1:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading KP predictions form model forward pass\n",
    "# Transform the image (assuming you have a transform function defined)\n",
    "x = transform(im)\n",
    "\n",
    "# Get model output\n",
    "# Assuming the model is already defined and loaded\n",
    "out = model(x.unsqueeze(0))\n",
    "\n",
    "# Convert the model output to a format suitable for plotting\n",
    "keypoints = out[0].detach().cpu().numpy()  # Detach and move to CPU if using a GPU\n",
    "\n",
    "# Access the first row of keypoints since shape is (1, 24)\n",
    "keypoints = keypoints[0]\n",
    "\n",
    "print(keypoints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(im)\n",
    "# Loop through the keypoints and plot them\n",
    "for i in range(0, len(keypoints), 2):\n",
    "    x_coord = keypoints[i]\n",
    "    y_coord = keypoints[i + 1]\n",
    "    plt.scatter(x_coord, y_coord, s=10, c='blue', marker='x')  # Plot each keypoint\n",
    "for i in range(0, 23, 2):\n",
    "    plt.plot(labelsKepoints[i], labelsKepoints[i+1], 'ro')\n",
    "    plt.plot(predsKeypoints[i], predsKeypoints[i+1], 'yo', markerfacecolor='none', markersize=10) #allows us to plot x+y coordinate of each key point (i+1) and loops over the 24 keypoints, skipping every second step since we plot both x+y \n",
    " \n",
    "plt.title(\"Image with Key Points\")\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize loss curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train.py producing lossCurve in steps\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        #steps = [d_lossCurve['steps'][i] for i in indices]\n",
    "        steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        if metric == 'training_loss':\n",
    "            stepsPerEpoch = 49\n",
    "            steps = np.arange(61)\n",
    "\n",
    "            loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using train_epochs.py, producing lossCurve in epochs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d_lossCurve = pd.read_csv('lossCurve.csv')\n",
    "\n",
    "colors = {'training_loss': 'blue', 'validation_loss': 'orange'}  # Define colors for different metrics\n",
    "metric_labels = {'training_loss': 'Training Loss', 'validation_loss': 'Test Loss'}  # Rename metrics\n",
    "labels2 = {'validation_loss':'Test loss', 'training_loss':'Training loss'}\n",
    "for metric, color in colors.items():\n",
    "    indices = [i for i, m in enumerate(d_lossCurve['metric']) if m == metric]\n",
    "    #print(f\"Metric: {metric}, Indices: {indices}\")  # Debug print statement\n",
    "    if indices:\n",
    "        steps = [d_lossCurve['epoch'][i] for i in indices]\n",
    "        #steps = [i*2 for i, _ in enumerate(indices)]\n",
    "        loss = [d_lossCurve['training_loss'][i] for i in indices]\n",
    "        # if metric == 'training_loss':\n",
    "        #     stepsPerEpoch = 49\n",
    "        #     steps = np.arange(61)\n",
    "\n",
    "        #     loss = [np.mean(loss[i*stepsPerEpoch:(i+1)*stepsPerEpoch]) for i in steps]\n",
    "            \n",
    "        #print(f\"Steps: {steps}, Loss: {loss}\")  # Debug print statement\n",
    "        plt.plot(steps, loss, label=f'{labels2[metric]}', color=color)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "\n",
    "# Set the y-axis to scientific notation\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCK\n",
    "\n",
    "PCK measures the percentage of keypoints that are predicted correctly within a certain normalized distance from the ground truth keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Works for the original labels.csv file in the data repo\n",
    "\n",
    "def calculate_pck(labels, preds, alpha=0.2, reference_points=(4, 5)):\n",
    "    \"\"\"\n",
    "    Calculate PCK (Percentage of Correct Keypoints) for each image and identify incorrect keypoints.\n",
    "\n",
    "    Args:\n",
    "        labels (pd.DataFrame): Ground truth keypoints with image names.\n",
    "        preds (pd.DataFrame): Predicted keypoints with image names.\n",
    "        alpha (float): Threshold for PCK, typically 0.2.\n",
    "        reference_points (tuple): Indices of keypoints to use as reference for normalization.\n",
    "\n",
    "    Returns:\n",
    "        pck_results (dict): Dictionary of image_name to (PCK value, list of incorrect keypoints).\n",
    "    \"\"\"\n",
    "    pck_results = {}\n",
    "    num_keypoints = (labels.shape[1] - 1) // 2  # Assuming first column is 'image_name'\n",
    "\n",
    "    for _, row in labels.iterrows():\n",
    "        image_name = row['image_name']\n",
    "        label_keypoints = row.values[1:].reshape(num_keypoints, 2)  # Ground truth keypoints\n",
    "        print(label_keypoints)\n",
    "        \n",
    "        pred_row = preds.loc[preds['image_names'] == image_name]\n",
    "        if pred_row.empty:\n",
    "            print(f\"No predictions found for image {image_name}\")\n",
    "            continue\n",
    "        pred_keypoints = pred_row.values[0][1:-1].reshape(num_keypoints, 2)  # Predicted keypoints\n",
    "        print(pred_keypoints)\n",
    "\n",
    "        # Calculate reference distance (e.g., shoulder distance)\n",
    "        ref_distance = np.linalg.norm(label_keypoints[reference_points[0]] - label_keypoints[reference_points[1]])\n",
    "        threshold_distance = alpha * ref_distance\n",
    "\n",
    "        correct_keypoints = 0\n",
    "        incorrect_keypoints = []  # To track which keypoints are incorrect\n",
    "\n",
    "        for j in range(num_keypoints):\n",
    "            distance = np.linalg.norm(pred_keypoints[j] - label_keypoints[j])\n",
    "            if distance < threshold_distance:\n",
    "                correct_keypoints += 1\n",
    "            else:\n",
    "                incorrect_keypoints.append(j)  # Store the index of the incorrect keypoint\n",
    "\n",
    "        pck = correct_keypoints / num_keypoints\n",
    "        pck_results[image_name] = (pck, incorrect_keypoints)  # Store PCK and incorrect keypoints\n",
    "\n",
    "    return pck_results\n",
    "\n",
    "# Load the data\n",
    "labels = pd.read_csv('augmented_data_test/augmented_labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK - automatically use the distance between eye_top and eye_bottom (which I expect to be quite stable) as reference pair (1,3).\n",
    "# KP should be within 10% of the distance between the ref pair\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKS ON THE PRODUCED LABELS\n",
    "\n",
    "import pandas as pd\n",
    "from utils.performance_metrics import calculate_pck\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Print results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.performance_metrics import calculate_pck, calculate_rmse, average_pck, average_rmse\n",
    "# Load the data\n",
    " #Usage example\n",
    "# Load the data\n",
    "labels = pd.read_csv('labels.csv')\n",
    "preds = pd.read_csv('predictions.csv')\n",
    "\n",
    "# Calculate PCK\n",
    "pck_results = calculate_pck(labels, preds, alpha=0.1, reference_points=(1, 3))\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse_results = calculate_rmse(labels, preds)\n",
    "\n",
    "# Print PCK and RMSE results\n",
    "for image_name, (pck_value, incorrect_keypoints) in pck_results.items():\n",
    "    rmse_value = rmse_results.get(image_name, None)\n",
    "    print(f\"Image: {image_name}, PCK: {pck_value:.2f}, Incorrect Keypoints: {incorrect_keypoints}, RMSE: {rmse_value:.4f}\")\n",
    "\n",
    "# Calculate and print average PCK and RMSE\n",
    "avg_pck = average_pck(pck_results)\n",
    "avg_rmse = average_rmse(rmse_results)\n",
    "print(f\"Average PCK: {avg_pck:.2f}, Average RMSE: {avg_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
