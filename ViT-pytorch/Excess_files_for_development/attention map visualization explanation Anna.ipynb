{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from models.modeling import VisionTransformer, CONFIGS\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (embeddings): Embeddings(\n",
      "      (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (attention_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (ffn): Mlp(\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (attn): Attention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (proj_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (encoder_norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (head): Linear(in_features=768, out_features=24, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 228])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Prepare Model\n",
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_classes=24, zero_head=False, img_size=224, vis=True)\n",
    "checkpoint = torch.load(\"output/test_checkpoint.pth\",map_location=torch.device('cpu'))  # Load the checkpoint #delete map_location=torch.device('cpu') if run on GPU\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(model)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "imageName = 'img6725_rotate_rescale_augmented.jpg'\n",
    "im = Image.open(\"augmented_data_test/\" + imageName)\n",
    "x = transform(im)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out the logits (i believe key points) from the model, and the attention maps. x.unsqueeze(0) adds a new dimension at position 0, effectively converting x into a batch with a single item. If x originally had the shape [C, H, W] (where C is channels, H is height, W is width), after unsqueeze(0), it would have the shape [1, C, H, W]. model(...) runs the input through the Vision Transformer model.\n",
    "#The model returns two outputs: logits: The raw output from the model's head (likely KP scores). att_mat: The attention matrices from the attention layers in the model. This is usually a list of attention maps from different layers of the transformer.\n",
    "logits, att_mat = model(x.unsqueeze(0))\n",
    "\n",
    "#torch.stack(att_mat) converts this list of matrices into a single tensor by stacking them along a new dimension. If att_mat is a list of tensors with shapes [B, N, N] (where B is batch size, and N is the number of tokens), then torch.stack(att_mat) will have shape [L, B, N, N] where L is the number of layers.\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "#torch.Size([12, 12, 197, 197])\n",
    "\n",
    "# Average the attention weights across all heads.\n",
    "#second dimension(1) (12): This represents the number of attention heads in each transformer layer. Each layer in a multi-head attention mechanism typically has multiple heads (in this case, 12), and each head produces its own attention matrix.\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "# This function creates a 2D identity matrix of size 197 x 197. In an identity matrix, all the diagonal elements are 1, and all off-diagonal elements are 0. This matrix represents a situation where each token attends only to itself with full attention (a residual connection).\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "\n",
    "\n",
    "# When you add residual_att (which has shape [197, 197]) to att_mat (which has shape [12, 12, 197, 197]), PyTorch automatically broadcasts the residual_att matrix across the first two dimensions (12 layers and 12 heads).\n",
    "# This means that residual_att is added to each [197, 197] attention matrix in att_mat.\n",
    "# Result (aug_att_mat):\n",
    "# Shape: [12, 12, 197, 197]\n",
    "# The resulting aug_att_mat tensor now contains the original attention values from att_mat plus 1s along the diagonal for each of the attention matrices (due to the addition of the identity matrix from residual_att).\n",
    "# This effectively means that each token has a stronger self-attention component since the diagonal elements (representing self-attention) have been incremented by 1.\n",
    "# Why Do This?\n",
    "# Adding residual_att to att_mat augments the attention matrices by reinforcing the self-attention mechanism. This can help stabilize training and ensure that each token's original information is preserved across layers. In essence, even after the attention mechanism mixes information from other tokens, each token retains some of its original identity (as reflected by the diagonal elements).\n",
    "aug_att_mat = att_mat + residual_att\n",
    "\n",
    "\n",
    "#THIS STEP I AM MORE IN DOUBT ABOUT\n",
    "# Step-by-Step Explanation:\n",
    "# aug_att_mat.sum(dim=-1):\n",
    "# dim=-1 refers to the last dimension of aug_att_mat, which is 197 (the sequence length, or the number of tokens).\n",
    "# The sum(dim=-1) operation sums the elements along this last dimension for each attention matrix.\n",
    "# The result is a tensor of shape [12, 12, 197], where each element is the sum of the attention scores for a particular token across all other tokens (including itself).\n",
    "# .unsqueeze(-1):\n",
    "# This adds a new dimension at the end of the tensor resulting from the sum operation.\n",
    "# The shape of the tensor changes from [12, 12, 197] to [12, 12, 197, 1].\n",
    "# This is necessary for broadcasting during the division operation that follows.\n",
    "# aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1):\n",
    "# Here, element-wise division is performed between aug_att_mat and the summed attention scores.\n",
    "# The broadcasting mechanism allows the [12, 12, 197, 1] tensor to be divided across the [12, 12, 197, 197] tensor.\n",
    "# This operation normalizes each attention matrix along its last dimension so that the sum of attention scores for each token across all tokens (including itself) equals 1.\n",
    "\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "# joint_attentions = torch.zeros(aug_att_mat.size()):\n",
    "# Creates a tensor joint_attentions initialized to zeros with the same size as aug_att_mat, which is [12, 12, 197, 197].\n",
    "# This tensor will store the cumulative attention matrices for each layer.\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "\n",
    "# joint_attentions[0] = aug_att_mat[0]:\n",
    "# The first entry in joint_attentions (corresponding to the first layer) is set equal to the first augmented attention matrix (aug_att_mat[0]).\n",
    "# This means that for the first layer, the joint attention is simply the attention of that layer.\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "\n",
    "\n",
    "# . Recursive Multiplication:\n",
    "\n",
    "# for n in range(1, aug_att_mat.size(0)):\n",
    "# Iterates over each layer starting from the second one (n=1) to the last one (n=11).\n",
    "# joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1]):\n",
    "# For each layer n, the code multiplies the current layer's attention matrix (aug_att_mat[n]) by the cumulative attention matrix from the previous layer (joint_attentions[n-1]).\n",
    "# torch.matmul(...): This performs matrix multiplication, which combines the current attention matrix with the joint attention from the previous layer.\n",
    "# The result is stored in joint_attentions[n].\n",
    "# 3. What This Achieves:\n",
    "\n",
    "# Cumulative Attention Across Layers:\n",
    "# The matrix multiplication accumulates the effects of attention across multiple layers.\n",
    "# joint_attentions[n] captures how attention is distributed from the input to the current layer n through the entire sequence of preceding layers.\n",
    "# By the time you reach the last layer, joint_attentions[-1] will represent the overall attention map that considers the cumulative effect of all layers.\n",
    "# Summary:\n",
    "# joint_attentions is a tensor that stores the cumulative attention maps for each layer in the Vision Transformer.\n",
    "# The recursive multiplication effectively traces how attention flows through the network from the input layer to each subsequent layer, providing a way to visualize or analyze the overall attention mechanism across the entire model.\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[-1]\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "\n",
    "\n",
    "#mask * im: Multiplies the attention mask with the original image im, element-wise. This highlights the regions of the image that the model attends to, with more attention resulting in brighter areas.\n",
    "result = (mask * im).astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "\n",
    "ax1.set_title('Original')\n",
    "ax2.set_title('Attention Map')\n",
    "_ = ax1.imshow(im)\n",
    "#_ = ax2.imshow(result)\n",
    "_ = ax2.imshow(mask) #trying to use mask instead so it doesn't just show the bright areas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over attention layers\n",
    "\n",
    "for i, v in enumerate(joint_attentions):\n",
    "    # Attention from the output token to the input space.\n",
    "    mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "    mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "    #result = (mask * im).astype(\"uint8\")\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "    ax1.set_title('Original')\n",
    "    ax2.set_title('Attention Map_%d Layer' % (i+1))\n",
    "    _ = ax1.imshow(im)\n",
    "    #_ = ax2.imshow(result)\n",
    "    _ = ax2.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each KP individually\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "logits, att_mat = model(x.unsqueeze(0))\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[14, 14]' is invalid for input of size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m key_point_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey Point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkp_index\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_index, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(joint_attentions):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Attention from the output token to the input space\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     mask_x \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;241m0\u001b[39m, kp_index \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(grid_size, grid_size)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      9\u001b[0m     mask_y \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;241m0\u001b[39m, kp_index \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(grid_size, grid_size)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Average the attention masks for the x and y coordinates\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[14, 14]' is invalid for input of size 1"
     ]
    }
   ],
   "source": [
    "# Assuming `im` is the image you want to visualize the attention on\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))  # Assuming grid size 14x14 for 197 tokens\n",
    "\n",
    "for kp_index in range(12):  # 12 keypoints (each with x, y)\n",
    "    key_point_name = f\"Key Point {kp_index + 1}\"\n",
    "    for layer_index, v in enumerate(joint_attentions):\n",
    "        # Attention from the output token to the input space\n",
    "        mask_x = v[0, kp_index * 2 + 1].reshape(grid_size, grid_size).detach().cpu().numpy()\n",
    "        mask_y = v[0, kp_index * 2 + 2].reshape(grid_size, grid_size).detach().cpu().numpy()\n",
    "        \n",
    "        # Average the attention masks for the x and y coordinates\n",
    "        mask = (mask_x + mask_y) / 2.0\n",
    "        mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "        result = (mask * im).astype(\"uint8\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n",
    "        ax1.set_title(f'Original - {key_point_name}')\n",
    "        ax2.set_title(f'Attention Map for {key_point_name} - Layer {layer_index + 1}')\n",
    "        _ = ax1.imshow(im)\n",
    "        _ = ax2.imshow(result)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
