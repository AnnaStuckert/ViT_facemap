{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"numpy<2\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class KeyPointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeyPointModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 24)  # 12 key points (24 coordinates) - add fuly connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avs20\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\avs20\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Instantiate the model\n",
    "model = KeyPointModel()\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "# Dataset Definition\n",
    "class KeyPointDataset(Dataset):\n",
    "    def __init__(self, image_paths, key_points, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.key_points = key_points\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        key_point = self.key_points[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, key_point\n",
    "\n",
    "# Function to load dataset from a specified folder\n",
    "def load_dataset(folder_path):\n",
    "    # Load key points from the CSV file\n",
    "    csv_file = os.path.join(folder_path, 'augmented_labels.csv')  # Change to your CSV file name\n",
    "    keypoint_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create lists for images and key points\n",
    "    image_paths = []\n",
    "    key_points = []\n",
    "\n",
    "    # Iterate through the CSV to get paths and key points\n",
    "    for index, row in keypoint_data.iterrows():\n",
    "        image_name = row['image_name']  # Replace with the actual column name in your CSV\n",
    "        keypoint = row[1:].values.astype(float)  # Assuming the first column is the image name\n",
    "        image_path = os.path.join(folder_path, f\"{image_name}\")  # Assuming images are in PNG format\n",
    "\n",
    "        image_paths.append(image_path)\n",
    "        key_points.append(torch.tensor(keypoint))\n",
    "\n",
    "    return image_paths, key_points\n",
    "\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Paths to your train and test folders\n",
    "train_folder = os.path.join(base_path, 'train', 'augmented_data')\n",
    "test_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Load datasets\n",
    "train_image_paths, train_key_points = load_dataset(train_folder)\n",
    "test_image_paths, test_key_points = load_dataset(test_folder)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = KeyPointDataset(train_image_paths, train_key_points, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "test_dataset = KeyPointDataset(test_image_paths, test_key_points, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manna-stuckert\u001b[0m (\u001b[33manna-stuckert-university-of-st-andrews\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\avs20\\Documents\\GitHub\\ViT_facemap\\ResNet50\\wandb\\run-20240927_130721-3ah26sbq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/3ah26sbq' target=\"_blank\">10_epochs</a></strong> to <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet' target=\"_blank\">https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/3ah26sbq' target=\"_blank\">https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/3ah26sbq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [0/45], Loss: 10774.8916\n",
      "Epoch [1/50], Batch [10/45], Loss: 8856.8779\n",
      "Epoch [1/50], Batch [20/45], Loss: 5473.4971\n",
      "Epoch [1/50], Batch [30/45], Loss: 3308.6038\n",
      "Epoch [1/50], Batch [40/45], Loss: 1973.7061\n",
      "Epoch [1/50], Average Loss: 5516.5233\n",
      "Epoch [2/50], Batch [0/45], Loss: 1423.7258\n",
      "Epoch [2/50], Batch [10/45], Loss: 852.3307\n",
      "Epoch [2/50], Batch [20/45], Loss: 449.5571\n",
      "Epoch [2/50], Batch [30/45], Loss: 282.5428\n",
      "Epoch [2/50], Batch [40/45], Loss: 271.3300\n",
      "Epoch [2/50], Average Loss: 588.4402\n",
      "Epoch [3/50], Batch [0/45], Loss: 85.2262\n",
      "Epoch [3/50], Batch [10/45], Loss: 56.2999\n",
      "Epoch [3/50], Batch [20/45], Loss: 277.9798\n",
      "Epoch [3/50], Batch [30/45], Loss: 288.2138\n",
      "Epoch [3/50], Batch [40/45], Loss: 26.2006\n",
      "Epoch [3/50], Average Loss: 102.4812\n",
      "Epoch [4/50], Batch [0/45], Loss: 220.0223\n",
      "Epoch [4/50], Batch [10/45], Loss: 28.1806\n",
      "Epoch [4/50], Batch [20/45], Loss: 29.0820\n",
      "Epoch [4/50], Batch [30/45], Loss: 38.3321\n",
      "Epoch [4/50], Batch [40/45], Loss: 28.4790\n",
      "Epoch [4/50], Average Loss: 67.1124\n",
      "Epoch [5/50], Batch [0/45], Loss: 114.4573\n",
      "Epoch [5/50], Batch [10/45], Loss: 24.6755\n",
      "Epoch [5/50], Batch [20/45], Loss: 66.2485\n",
      "Epoch [5/50], Batch [30/45], Loss: 32.9527\n",
      "Epoch [5/50], Batch [40/45], Loss: 40.5068\n",
      "Epoch [5/50], Average Loss: 58.5934\n",
      "Epoch [6/50], Batch [0/45], Loss: 36.7321\n",
      "Epoch [6/50], Batch [10/45], Loss: 35.1578\n",
      "Epoch [6/50], Batch [20/45], Loss: 51.0573\n",
      "Epoch [6/50], Batch [30/45], Loss: 39.4591\n",
      "Epoch [6/50], Batch [40/45], Loss: 88.7239\n",
      "Epoch [6/50], Average Loss: 65.8278\n",
      "Epoch [7/50], Batch [0/45], Loss: 23.9986\n",
      "Epoch [7/50], Batch [10/45], Loss: 52.4712\n",
      "Epoch [7/50], Batch [20/45], Loss: 51.6053\n",
      "Epoch [7/50], Batch [30/45], Loss: 115.0344\n",
      "Epoch [7/50], Batch [40/45], Loss: 26.8321\n",
      "Epoch [7/50], Average Loss: 35.9212\n",
      "Epoch [8/50], Batch [0/45], Loss: 39.7376\n",
      "Epoch [8/50], Batch [10/45], Loss: 63.1842\n",
      "Epoch [8/50], Batch [20/45], Loss: 30.1968\n",
      "Epoch [8/50], Batch [30/45], Loss: 91.2739\n",
      "Epoch [8/50], Batch [40/45], Loss: 30.0322\n",
      "Epoch [8/50], Average Loss: 39.1563\n",
      "Epoch [9/50], Batch [0/45], Loss: 24.6070\n",
      "Epoch [9/50], Batch [10/45], Loss: 29.1052\n",
      "Epoch [9/50], Batch [20/45], Loss: 28.1750\n",
      "Epoch [9/50], Batch [30/45], Loss: 62.9946\n",
      "Epoch [9/50], Batch [40/45], Loss: 190.0502\n",
      "Epoch [9/50], Average Loss: 30.9136\n",
      "Epoch [10/50], Batch [0/45], Loss: 20.1996\n",
      "Epoch [10/50], Batch [10/45], Loss: 28.4145\n",
      "Epoch [10/50], Batch [20/45], Loss: 22.0018\n",
      "Epoch [10/50], Batch [30/45], Loss: 54.0074\n",
      "Epoch [10/50], Batch [40/45], Loss: 43.0136\n",
      "Epoch [10/50], Average Loss: 26.6207\n",
      "Epoch [11/50], Batch [0/45], Loss: 20.0104\n",
      "Epoch [11/50], Batch [10/45], Loss: 45.4559\n",
      "Epoch [11/50], Batch [20/45], Loss: 24.2989\n",
      "Epoch [11/50], Batch [30/45], Loss: 18.6377\n",
      "Epoch [11/50], Batch [40/45], Loss: 24.0042\n",
      "Epoch [11/50], Average Loss: 28.7833\n",
      "Epoch [12/50], Batch [0/45], Loss: 35.8154\n",
      "Epoch [12/50], Batch [10/45], Loss: 20.1804\n",
      "Epoch [12/50], Batch [20/45], Loss: 56.0513\n",
      "Epoch [12/50], Batch [30/45], Loss: 32.5313\n",
      "Epoch [12/50], Batch [40/45], Loss: 19.5043\n",
      "Epoch [12/50], Average Loss: 27.1455\n",
      "Epoch [13/50], Batch [0/45], Loss: 16.5983\n",
      "Epoch [13/50], Batch [10/45], Loss: 15.9045\n",
      "Epoch [13/50], Batch [20/45], Loss: 28.3578\n",
      "Epoch [13/50], Batch [30/45], Loss: 27.2717\n",
      "Epoch [13/50], Batch [40/45], Loss: 17.9377\n",
      "Epoch [13/50], Average Loss: 27.4043\n",
      "Epoch [14/50], Batch [0/45], Loss: 28.4856\n",
      "Epoch [14/50], Batch [10/45], Loss: 18.7648\n",
      "Epoch [14/50], Batch [20/45], Loss: 75.3215\n",
      "Epoch [14/50], Batch [30/45], Loss: 18.8015\n",
      "Epoch [14/50], Batch [40/45], Loss: 22.7889\n",
      "Epoch [14/50], Average Loss: 25.9744\n",
      "Epoch [15/50], Batch [0/45], Loss: 20.3268\n",
      "Epoch [15/50], Batch [10/45], Loss: 14.5764\n",
      "Epoch [15/50], Batch [20/45], Loss: 19.5720\n",
      "Epoch [15/50], Batch [30/45], Loss: 14.1491\n",
      "Epoch [15/50], Batch [40/45], Loss: 14.2790\n",
      "Epoch [15/50], Average Loss: 18.6506\n",
      "Epoch [16/50], Batch [0/45], Loss: 16.2792\n",
      "Epoch [16/50], Batch [10/45], Loss: 22.9928\n",
      "Epoch [16/50], Batch [20/45], Loss: 15.5137\n",
      "Epoch [16/50], Batch [30/45], Loss: 12.7110\n",
      "Epoch [16/50], Batch [40/45], Loss: 15.9279\n",
      "Epoch [16/50], Average Loss: 15.8866\n",
      "Epoch [17/50], Batch [0/45], Loss: 19.2789\n",
      "Epoch [17/50], Batch [10/45], Loss: 11.3091\n",
      "Epoch [17/50], Batch [20/45], Loss: 12.1794\n",
      "Epoch [17/50], Batch [30/45], Loss: 16.4501\n",
      "Epoch [17/50], Batch [40/45], Loss: 17.8363\n",
      "Epoch [17/50], Average Loss: 21.2378\n",
      "Epoch [18/50], Batch [0/45], Loss: 12.6476\n",
      "Epoch [18/50], Batch [10/45], Loss: 40.9375\n",
      "Epoch [18/50], Batch [20/45], Loss: 18.7313\n",
      "Epoch [18/50], Batch [30/45], Loss: 11.3857\n",
      "Epoch [18/50], Batch [40/45], Loss: 13.4967\n",
      "Epoch [18/50], Average Loss: 17.4991\n",
      "Epoch [19/50], Batch [0/45], Loss: 9.4184\n",
      "Epoch [19/50], Batch [10/45], Loss: 9.8391\n",
      "Epoch [19/50], Batch [20/45], Loss: 12.0770\n",
      "Epoch [19/50], Batch [30/45], Loss: 8.9366\n",
      "Epoch [19/50], Batch [40/45], Loss: 10.8142\n",
      "Epoch [19/50], Average Loss: 12.5239\n",
      "Epoch [20/50], Batch [0/45], Loss: 13.2680\n",
      "Epoch [20/50], Batch [10/45], Loss: 16.7290\n",
      "Epoch [20/50], Batch [20/45], Loss: 8.2360\n",
      "Epoch [20/50], Batch [30/45], Loss: 17.9051\n",
      "Epoch [20/50], Batch [40/45], Loss: 12.8538\n",
      "Epoch [20/50], Average Loss: 13.7513\n",
      "Epoch [21/50], Batch [0/45], Loss: 11.9547\n",
      "Epoch [21/50], Batch [10/45], Loss: 8.2784\n",
      "Epoch [21/50], Batch [20/45], Loss: 14.8225\n",
      "Epoch [21/50], Batch [30/45], Loss: 8.7577\n",
      "Epoch [21/50], Batch [40/45], Loss: 16.2416\n",
      "Epoch [21/50], Average Loss: 15.6514\n",
      "Epoch [22/50], Batch [0/45], Loss: 16.7108\n",
      "Epoch [22/50], Batch [10/45], Loss: 13.3517\n",
      "Epoch [22/50], Batch [20/45], Loss: 16.6818\n",
      "Epoch [22/50], Batch [30/45], Loss: 10.7700\n",
      "Epoch [22/50], Batch [40/45], Loss: 9.0222\n",
      "Epoch [22/50], Average Loss: 13.3268\n",
      "Epoch [23/50], Batch [0/45], Loss: 6.6433\n",
      "Epoch [23/50], Batch [10/45], Loss: 8.0140\n",
      "Epoch [23/50], Batch [20/45], Loss: 9.3085\n",
      "Epoch [23/50], Batch [30/45], Loss: 8.5202\n",
      "Epoch [23/50], Batch [40/45], Loss: 14.7283\n",
      "Epoch [23/50], Average Loss: 9.4699\n",
      "Epoch [24/50], Batch [0/45], Loss: 7.5885\n",
      "Epoch [24/50], Batch [10/45], Loss: 9.2972\n",
      "Epoch [24/50], Batch [20/45], Loss: 7.1171\n",
      "Epoch [24/50], Batch [30/45], Loss: 6.7753\n",
      "Epoch [24/50], Batch [40/45], Loss: 7.8430\n",
      "Epoch [24/50], Average Loss: 9.7573\n",
      "Epoch [25/50], Batch [0/45], Loss: 6.1616\n",
      "Epoch [25/50], Batch [10/45], Loss: 5.9527\n",
      "Epoch [25/50], Batch [20/45], Loss: 12.5997\n",
      "Epoch [25/50], Batch [30/45], Loss: 8.1410\n",
      "Epoch [25/50], Batch [40/45], Loss: 8.3027\n",
      "Epoch [25/50], Average Loss: 9.5083\n",
      "Epoch [26/50], Batch [0/45], Loss: 30.0332\n",
      "Epoch [26/50], Batch [10/45], Loss: 7.2697\n",
      "Epoch [26/50], Batch [20/45], Loss: 6.2657\n",
      "Epoch [26/50], Batch [30/45], Loss: 28.7026\n",
      "Epoch [26/50], Batch [40/45], Loss: 8.5205\n",
      "Epoch [26/50], Average Loss: 11.0616\n",
      "Epoch [27/50], Batch [0/45], Loss: 5.5417\n",
      "Epoch [27/50], Batch [10/45], Loss: 7.0965\n",
      "Epoch [27/50], Batch [20/45], Loss: 5.3914\n",
      "Epoch [27/50], Batch [30/45], Loss: 7.5367\n",
      "Epoch [27/50], Batch [40/45], Loss: 11.2505\n",
      "Epoch [27/50], Average Loss: 8.0353\n",
      "Epoch [28/50], Batch [0/45], Loss: 5.4085\n",
      "Epoch [28/50], Batch [10/45], Loss: 8.6860\n",
      "Epoch [28/50], Batch [20/45], Loss: 5.4394\n",
      "Epoch [28/50], Batch [30/45], Loss: 7.4875\n",
      "Epoch [28/50], Batch [40/45], Loss: 5.7148\n",
      "Epoch [28/50], Average Loss: 7.6518\n",
      "Epoch [29/50], Batch [0/45], Loss: 5.5721\n",
      "Epoch [29/50], Batch [10/45], Loss: 6.2151\n",
      "Epoch [29/50], Batch [20/45], Loss: 8.6769\n",
      "Epoch [29/50], Batch [30/45], Loss: 4.7547\n",
      "Epoch [29/50], Batch [40/45], Loss: 4.9754\n",
      "Epoch [29/50], Average Loss: 7.9997\n",
      "Epoch [30/50], Batch [0/45], Loss: 6.1107\n",
      "Epoch [30/50], Batch [10/45], Loss: 7.9260\n",
      "Epoch [30/50], Batch [20/45], Loss: 5.9999\n",
      "Epoch [30/50], Batch [30/45], Loss: 5.2534\n",
      "Epoch [30/50], Batch [40/45], Loss: 7.5166\n",
      "Epoch [30/50], Average Loss: 8.3189\n",
      "Epoch [31/50], Batch [0/45], Loss: 5.4728\n",
      "Epoch [31/50], Batch [10/45], Loss: 8.9112\n",
      "Epoch [31/50], Batch [20/45], Loss: 14.2602\n",
      "Epoch [31/50], Batch [30/45], Loss: 7.5822\n",
      "Epoch [31/50], Batch [40/45], Loss: 9.8797\n",
      "Epoch [31/50], Average Loss: 7.6993\n",
      "Epoch [32/50], Batch [0/45], Loss: 4.6056\n",
      "Epoch [32/50], Batch [10/45], Loss: 18.1034\n",
      "Epoch [32/50], Batch [20/45], Loss: 9.4988\n",
      "Epoch [32/50], Batch [30/45], Loss: 5.9636\n",
      "Epoch [32/50], Batch [40/45], Loss: 12.6512\n",
      "Epoch [32/50], Average Loss: 7.1788\n",
      "Epoch [33/50], Batch [0/45], Loss: 12.2281\n",
      "Epoch [33/50], Batch [10/45], Loss: 6.6371\n",
      "Epoch [33/50], Batch [20/45], Loss: 4.9606\n",
      "Epoch [33/50], Batch [30/45], Loss: 5.5759\n",
      "Epoch [33/50], Batch [40/45], Loss: 6.3952\n",
      "Epoch [33/50], Average Loss: 6.4144\n",
      "Epoch [34/50], Batch [0/45], Loss: 5.4020\n",
      "Epoch [34/50], Batch [10/45], Loss: 5.6082\n",
      "Epoch [34/50], Batch [20/45], Loss: 5.9177\n",
      "Epoch [34/50], Batch [30/45], Loss: 6.8413\n",
      "Epoch [34/50], Batch [40/45], Loss: 4.9068\n",
      "Epoch [34/50], Average Loss: 6.5218\n",
      "Epoch [35/50], Batch [0/45], Loss: 14.5723\n",
      "Epoch [35/50], Batch [10/45], Loss: 4.4315\n",
      "Epoch [35/50], Batch [20/45], Loss: 6.7107\n",
      "Epoch [35/50], Batch [30/45], Loss: 4.8044\n",
      "Epoch [35/50], Batch [40/45], Loss: 4.2745\n",
      "Epoch [35/50], Average Loss: 6.0691\n",
      "Epoch [36/50], Batch [0/45], Loss: 5.4219\n",
      "Epoch [36/50], Batch [10/45], Loss: 14.4753\n",
      "Epoch [36/50], Batch [20/45], Loss: 4.9292\n",
      "Epoch [36/50], Batch [30/45], Loss: 24.1763\n",
      "Epoch [36/50], Batch [40/45], Loss: 5.0156\n",
      "Epoch [36/50], Average Loss: 7.2369\n",
      "Epoch [37/50], Batch [0/45], Loss: 6.1092\n",
      "Epoch [37/50], Batch [10/45], Loss: 3.8872\n",
      "Epoch [37/50], Batch [20/45], Loss: 20.0130\n",
      "Epoch [37/50], Batch [30/45], Loss: 3.2047\n",
      "Epoch [37/50], Batch [40/45], Loss: 3.9147\n",
      "Epoch [37/50], Average Loss: 7.0224\n",
      "Epoch [38/50], Batch [0/45], Loss: 4.8485\n",
      "Epoch [38/50], Batch [10/45], Loss: 4.6231\n",
      "Epoch [38/50], Batch [20/45], Loss: 5.0611\n",
      "Epoch [38/50], Batch [30/45], Loss: 7.0621\n",
      "Epoch [38/50], Batch [40/45], Loss: 7.3710\n",
      "Epoch [38/50], Average Loss: 8.5175\n",
      "Epoch [39/50], Batch [0/45], Loss: 4.8810\n",
      "Epoch [39/50], Batch [10/45], Loss: 4.6529\n",
      "Epoch [39/50], Batch [20/45], Loss: 6.5538\n",
      "Epoch [39/50], Batch [30/45], Loss: 4.6585\n",
      "Epoch [39/50], Batch [40/45], Loss: 6.9346\n",
      "Epoch [39/50], Average Loss: 6.4594\n",
      "Epoch [40/50], Batch [0/45], Loss: 3.9651\n",
      "Epoch [40/50], Batch [10/45], Loss: 4.3691\n",
      "Epoch [40/50], Batch [20/45], Loss: 5.9018\n",
      "Epoch [40/50], Batch [30/45], Loss: 3.7136\n",
      "Epoch [40/50], Batch [40/45], Loss: 3.8668\n",
      "Epoch [40/50], Average Loss: 8.3037\n",
      "Epoch [41/50], Batch [0/45], Loss: 5.8415\n",
      "Epoch [41/50], Batch [10/45], Loss: 6.3606\n",
      "Epoch [41/50], Batch [20/45], Loss: 13.9643\n",
      "Epoch [41/50], Batch [30/45], Loss: 4.4628\n",
      "Epoch [41/50], Batch [40/45], Loss: 3.9533\n",
      "Epoch [41/50], Average Loss: 6.6883\n",
      "Epoch [42/50], Batch [0/45], Loss: 4.7048\n",
      "Epoch [42/50], Batch [10/45], Loss: 26.2044\n",
      "Epoch [42/50], Batch [20/45], Loss: 9.1864\n",
      "Epoch [42/50], Batch [30/45], Loss: 5.1790\n",
      "Epoch [42/50], Batch [40/45], Loss: 7.0702\n",
      "Epoch [42/50], Average Loss: 12.4349\n",
      "Epoch [43/50], Batch [0/45], Loss: 6.3986\n",
      "Epoch [43/50], Batch [10/45], Loss: 4.4709\n",
      "Epoch [43/50], Batch [20/45], Loss: 5.7815\n",
      "Epoch [43/50], Batch [30/45], Loss: 4.7142\n",
      "Epoch [43/50], Batch [40/45], Loss: 10.4742\n",
      "Epoch [43/50], Average Loss: 7.2475\n",
      "Epoch [44/50], Batch [0/45], Loss: 12.7018\n",
      "Epoch [44/50], Batch [10/45], Loss: 4.6789\n",
      "Epoch [44/50], Batch [20/45], Loss: 9.1655\n",
      "Epoch [44/50], Batch [30/45], Loss: 6.6277\n",
      "Epoch [44/50], Batch [40/45], Loss: 3.5505\n",
      "Epoch [44/50], Average Loss: 6.1917\n",
      "Epoch [45/50], Batch [0/45], Loss: 4.8827\n",
      "Epoch [45/50], Batch [10/45], Loss: 4.2103\n",
      "Epoch [45/50], Batch [20/45], Loss: 4.4976\n",
      "Epoch [45/50], Batch [30/45], Loss: 5.3053\n",
      "Epoch [45/50], Batch [40/45], Loss: 3.8016\n",
      "Epoch [45/50], Average Loss: 6.2065\n",
      "Epoch [46/50], Batch [0/45], Loss: 5.6498\n",
      "Epoch [46/50], Batch [10/45], Loss: 6.9204\n",
      "Epoch [46/50], Batch [20/45], Loss: 5.3740\n",
      "Epoch [46/50], Batch [30/45], Loss: 4.8018\n",
      "Epoch [46/50], Batch [40/45], Loss: 5.2089\n",
      "Epoch [46/50], Average Loss: 6.4444\n",
      "Epoch [47/50], Batch [0/45], Loss: 3.6608\n",
      "Epoch [47/50], Batch [10/45], Loss: 4.2808\n",
      "Epoch [47/50], Batch [20/45], Loss: 5.6600\n",
      "Epoch [47/50], Batch [30/45], Loss: 29.7144\n",
      "Epoch [47/50], Batch [40/45], Loss: 19.5024\n",
      "Epoch [47/50], Average Loss: 15.7775\n",
      "Epoch [48/50], Batch [0/45], Loss: 25.9096\n",
      "Epoch [48/50], Batch [10/45], Loss: 12.3668\n",
      "Epoch [48/50], Batch [20/45], Loss: 23.7919\n",
      "Epoch [48/50], Batch [30/45], Loss: 11.5776\n",
      "Epoch [48/50], Batch [40/45], Loss: 18.4267\n",
      "Epoch [48/50], Average Loss: 19.0847\n",
      "Epoch [49/50], Batch [0/45], Loss: 11.5352\n",
      "Epoch [49/50], Batch [10/45], Loss: 10.5840\n",
      "Epoch [49/50], Batch [20/45], Loss: 7.3547\n",
      "Epoch [49/50], Batch [30/45], Loss: 9.0318\n",
      "Epoch [49/50], Batch [40/45], Loss: 7.7226\n",
      "Epoch [49/50], Average Loss: 12.2874\n",
      "Epoch [50/50], Batch [0/45], Loss: 9.2516\n",
      "Epoch [50/50], Batch [10/45], Loss: 11.1374\n",
      "Epoch [50/50], Batch [20/45], Loss: 4.8331\n",
      "Epoch [50/50], Batch [30/45], Loss: 4.7681\n",
      "Epoch [50/50], Batch [40/45], Loss: 5.0506\n",
      "Epoch [50/50], Average Loss: 8.0814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\avs20\\\\Documents\\\\GitHub\\\\ViT_facemap\\\\ResNet50\\\\wandb\\\\run-20240927_130721-3ah26sbq\\\\files\\\\keypoint_model.pth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"Facemap_ResNet\", name=\"10_epochs\")\n",
    "\n",
    "# Define your hyperparameters and log them to WandB\n",
    "config = wandb.config\n",
    "config.num_epochs = 50\n",
    "config.learning_rate = 0.001\n",
    "config.batch_size = 20\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 50  # Number of epochs\n",
    "config.num_epochs = num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        # Move data to the GPU\n",
    "        images = images.to(device)\n",
    "        targets = targets.float().to(device)  \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log the loss for the batch to WandB\n",
    "        wandb.log({\"Batch Loss\": loss.item()})\n",
    "        \n",
    "        # Print progress every 10 batches (adjust as needed)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "    wandb.log({\"Epoch\": epoch + 1, \"Average Loss\": avg_loss})\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'keypoint_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save the model to WandB\n",
    "wandb.save(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting predictions via forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('keypoint_model.pth'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(image_path, keypoints):\n",
    "    # Load and display the original image\n",
    "    original_image = Image.open(image_path)\n",
    "    plt.imshow(original_image)\n",
    "    \n",
    "    # Convert keypoints to a numpy array and plot them\n",
    "    x_coords = keypoints[0::2]\n",
    "    y_coords = keypoints[1::2]\n",
    "    plt.scatter(x_coords, y_coords, c='red', marker='x')\n",
    "    plt.title('Predicted Key Points')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Path to the image you want to predict key points for\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Directory containing the images\n",
    "image_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Get the first image in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "if not image_files:\n",
    "    raise ValueError(\"No images found in the specified folder.\")\n",
    "image_path = os.path.join(image_folder, image_files[0])\n",
    "\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Perform a forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    predicted_keypoints = model(input_image)\n",
    "\n",
    "# Convert predictions to numpy array and detach from the graph\n",
    "predicted_keypoints = predicted_keypoints.squeeze().numpy()\n",
    "\n",
    "# Visualize the predicted key points\n",
    "visualize_predictions(image_path, predicted_keypoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP maybe works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap\n",
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('keypoint_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Prepare the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to prepare input for SHAP\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Directory containing the images\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Directory containing the images\n",
    "image_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Get the first image in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "if not image_files:\n",
    "    raise ValueError(\"No images found in the specified folder.\")\n",
    "first_image_path = os.path.join(image_folder, image_files[0])\n",
    "\n",
    "# Prepare the image for SHAP analysis\n",
    "images = preprocess_image(first_image_path)\n",
    "\n",
    "# Reshape images to 2D (num_samples, num_features)\n",
    "images_reshaped = images.view(images.size(0), -1)  # Flatten each image\n",
    "\n",
    "# Create a SHAP explainer\n",
    "def model_predict(input_data):\n",
    "    input_data = torch.tensor(input_data, dtype=torch.float32)  # Ensure input is a tensor\n",
    "    with torch.no_grad():\n",
    "        return model(input_data.view(-1, 3, 224, 224)).numpy()  # Reshape back to original image size\n",
    "\n",
    "# Initialize the Kernel Explainer with flattened images\n",
    "explainer = shap.KernelExplainer(model_predict, images_reshaped.numpy())\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(images_reshaped.numpy())\n",
    "\n",
    "# Visualize SHAP values for the first image\n",
    "shap.initjs()\n",
    "\n",
    "# Remove the batch dimension for visualization\n",
    "images_for_plotting = images.squeeze(0).permute(1, 2, 0).numpy()  # Shape to (224, 224, 3)\n",
    "\n",
    "shap.image_plot(shap_values, images_for_plotting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP based on each KP\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('keypoint_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Function to load dataset from a specified folder\n",
    "def load_dataset(folder_path):\n",
    "    # Load key points from the CSV file\n",
    "    csv_file = os.path.join(folder_path, 'augmented_labels.csv')  # Change to your CSV file name\n",
    "    keypoint_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create lists for images and key points\n",
    "    image_paths = []\n",
    "    key_points = []\n",
    "\n",
    "    # Iterate through the CSV to get paths and key points\n",
    "    for index, row in keypoint_data.iterrows():\n",
    "        image_name = row['image_name']  # Replace with the actual column name in your CSV\n",
    "        keypoint = row[1:].values.astype(float)  # Assuming the first column is the image name\n",
    "        image_path = os.path.join(folder_path, f\"{image_name}\")  # Assuming images are in PNG format\n",
    "\n",
    "        image_paths.append(image_path)\n",
    "        key_points.append(torch.tensor(keypoint))\n",
    "\n",
    "    return image_paths, key_points\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to model's input size\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet\n",
    "    ])\n",
    "    \n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        image = Image.open(path).convert('RGB')  # Load image and convert to RGB\n",
    "        image = transform(image)  # Apply transformations\n",
    "        images.append(image)\n",
    "    \n",
    "    return torch.stack(images)  # Stack images into a single tensor\n",
    "\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Paths to your train and test folders\n",
    "train_folder = os.path.join(base_path, 'train', 'augmented_data')\n",
    "test_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Load datasets\n",
    "train_image_paths, train_key_points = load_dataset(train_folder)\n",
    "test_image_paths, test_key_points = load_dataset(test_folder)\n",
    "\n",
    "# Load and preprocess images\n",
    "X_train = load_and_preprocess_images(train_image_paths)\n",
    "X_test = load_and_preprocess_images(test_image_paths)\n",
    "\n",
    "# Convert key points to tensors\n",
    "y_train = torch.stack(train_key_points)\n",
    "y_test = torch.stack(test_key_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Assuming 'X_train' is your training data\n",
    "# Ensure that X_train is a PyTorch tensor and move it to the GPU\n",
    "X_train_tensor = torch.tensor(X_train).to(device)\n",
    "\n",
    "# Use a very small sample of data to avoid OOM error\n",
    "X_train_sample = X_train_tensor[:10]  # Adjust based on your needs\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create the SHAP explainer\n",
    "explainer = shap.GradientExplainer(model.cpu(), X_train_sample.cpu())  # Use CPU for SHAP\n",
    "\n",
    "# Use torch.no_grad() to save memory during SHAP value computation\n",
    "with torch.no_grad():\n",
    "    shap_values = explainer.shap_values(X_train_sample.cpu())\n",
    "\n",
    "# Convert SHAP values back to numpy if needed\n",
    "shap_values_np = [val.detach().numpy() for val in shap_values]\n",
    "\n",
    "# Now you can use shap_values_np for further analysis or visualization\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'X_test' is your test data for which you want explanations\n",
    "shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values in smaller batches\n",
    "shap_values = []\n",
    "batch_size = 1  # Adjust this value based on your available memory\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, X_test.shape[0], batch_size):\n",
    "        batch_shap_values = explainer.shap_values(X_test[i:i + batch_size])\n",
    "        shap_values.extend(batch_shap_values)\n",
    "        torch.cuda.empty_cache()  # Clear memory after each batch if necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):  # Loop through each key point\n",
    "    shap.initjs()\n",
    "    shap.summary_plot(shap_values[i], X_test, feature_names=your_feature_names)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
