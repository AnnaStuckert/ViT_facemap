{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class KeyPointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeyPointModel, self).__init__()\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 24)  # 12 key points (24 coordinates) - add fuly connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annastuckert/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/annastuckert/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Instantiate the model\n",
    "model = KeyPointModel()\n",
    "model.train()  # Set to training mode\n",
    "\n",
    "# Dataset Definition\n",
    "class KeyPointDataset(Dataset):\n",
    "    def __init__(self, image_paths, key_points, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.key_points = key_points\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        key_point = self.key_points[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, key_point\n",
    "\n",
    "# Function to load dataset from a specified folder\n",
    "def load_dataset(folder_path):\n",
    "    # Load key points from the CSV file\n",
    "    csv_file = os.path.join(folder_path, 'augmented_labels.csv')  # Change to your CSV file name\n",
    "    keypoint_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create lists for images and key points\n",
    "    image_paths = []\n",
    "    key_points = []\n",
    "\n",
    "    # Iterate through the CSV to get paths and key points\n",
    "    for index, row in keypoint_data.iterrows():\n",
    "        image_name = row['image_name']  # Replace with the actual column name in your CSV\n",
    "        keypoint = row[1:].values.astype(float)  # Assuming the first column is the image name\n",
    "        image_path = os.path.join(folder_path, f\"{image_name}\")  # Assuming images are in PNG format\n",
    "\n",
    "        image_paths.append(image_path)\n",
    "        key_points.append(torch.tensor(keypoint))\n",
    "\n",
    "    return image_paths, key_points\n",
    "\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Paths to your train and test folders\n",
    "train_folder = os.path.join(base_path, 'train', 'augmented_data')\n",
    "test_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Load datasets\n",
    "train_image_paths, train_key_points = load_dataset(train_folder)\n",
    "test_image_paths, test_key_points = load_dataset(test_folder)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = KeyPointDataset(train_image_paths, train_key_points, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "test_dataset = KeyPointDataset(test_image_paths, test_key_points, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manna-stuckert\u001b[0m (\u001b[33manna-stuckert-university-of-st-andrews\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/annastuckert/Documents/GitHub/ViT_facemap/ResNet50/wandb/run-20241016_153247-10ygy75b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/10ygy75b' target=\"_blank\">50_epochs</a></strong> to <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet' target=\"_blank\">https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/10ygy75b' target=\"_blank\">https://wandb.ai/anna-stuckert-university-of-st-andrews/Facemap_ResNet/runs/10ygy75b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Batch [0/45], Loss: 10793.7461\n",
      "Epoch [1/50], Batch [10/45], Loss: 7963.4580\n",
      "Epoch [1/50], Batch [20/45], Loss: 5120.9150\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set the model to training mode\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;66;03m# Move data to the GPU\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m, in \u001b[0;36mKeyPointDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 25\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx])\n\u001b[1;32m     26\u001b[0m     key_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_points[idx]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/PIL/Image.py:3236\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3233\u001b[0m     fp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO(fp\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m   3234\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 3236\u001b[0m prefix \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m   3238\u001b[0m preinit()\n\u001b[1;32m   3240\u001b[0m accept_warnings \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This code should include both validation and training loss \n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "# Initialize WandB\n",
    "wandb.init(project=\"Facemap_ResNet\", name=\"50_epochs\")\n",
    "\n",
    "# Define your hyperparameters and log them to WandB\n",
    "config = wandb.config\n",
    "config.num_epochs = 50\n",
    "config.learning_rate = 0.001\n",
    "config.batch_size = 20\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer, scheduler, and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = config.num_epochs  # Number of epochs\n",
    "global_step = 0  # To keep track of total steps across epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Training loop\n",
    "    for batch_idx, (images, targets) in enumerate(train_dataloader):\n",
    "        # Move data to the GPU\n",
    "        images = images.to(device)\n",
    "        targets = targets.float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        global_step += 1  # Increment global step\n",
    "\n",
    "        # Log the loss for the batch to WandB\n",
    "        wandb.log({\"Batch Loss\": loss.item(), \"global_step\": global_step})\n",
    "\n",
    "        # Print progress every 10 batches (adjust as needed)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # Calculate average training loss for the epoch\n",
    "    avg_train_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}')\n",
    "    \n",
    "    # Validation loop after each epoch\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(test_dataloader):\n",
    "            # Move data to the GPU\n",
    "            images = images.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            val_loss = criterion(outputs, targets)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "    # Calculate average validation loss for the epoch\n",
    "    avg_val_loss = val_running_loss / len(test_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Log epoch-level metrics (training and validation loss) to WandB\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Average Training Loss\": avg_train_loss,\n",
    "        \"Average Validation Loss\": avg_val_loss\n",
    "    })\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'R50_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save the model to WandB\n",
    "wandb.save(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting predictions via forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('R50_model.pth'))\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    transform = transforms.Compose([\n",
    "        #transforms.Resize(256),\n",
    "        #transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to visualize predictions\n",
    "def visualize_predictions(image_path, keypoints):\n",
    "    # Load and display the original image\n",
    "    original_image = Image.open(image_path)\n",
    "    plt.imshow(original_image)\n",
    "    \n",
    "    # Convert keypoints to a numpy array and plot them\n",
    "    x_coords = keypoints[0::2]\n",
    "    y_coords = keypoints[1::2]\n",
    "    plt.scatter(x_coords, y_coords, c='red', marker='x')\n",
    "    plt.title('Predicted Key Points')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Path to the image you want to predict key points for\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Directory containing the images\n",
    "image_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Get the first image in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "if not image_files:\n",
    "    raise ValueError(\"No images found in the specified folder.\")\n",
    "image_path = os.path.join(image_folder, image_files[0])\n",
    "\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Perform a forward pass to get predictions\n",
    "with torch.no_grad():\n",
    "    predicted_keypoints = model(input_image)\n",
    "\n",
    "# Convert predictions to numpy array and detach from the graph\n",
    "predicted_keypoints = predicted_keypoints.squeeze().numpy()\n",
    "\n",
    "# Visualize the predicted key points\n",
    "visualize_predictions(image_path, predicted_keypoints)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From hereon I was trying to make the SHAP explainer work - for the time being I have kept the code here for reference, but the 'working' kernel explainer is in the script kernelSHAP.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP maybe works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap\n",
    "import os\n",
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('keypoint_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Prepare the transform\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize(256),\n",
    "    #transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to prepare input for SHAP\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Directory containing the images\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Directory containing the images\n",
    "image_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Get the first image in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "if not image_files:\n",
    "    raise ValueError(\"No images found in the specified folder.\")\n",
    "first_image_path = os.path.join(image_folder, image_files[0])\n",
    "\n",
    "# Prepare the image for SHAP analysis\n",
    "images = preprocess_image(first_image_path)\n",
    "\n",
    "# Reshape images to 2D (num_samples, num_features)\n",
    "images_reshaped = images.view(images.size(0), -1)  # Flatten each image\n",
    "\n",
    "# Create a SHAP explainer\n",
    "def model_predict(input_data):\n",
    "    input_data = torch.tensor(input_data, dtype=torch.float32)  # Ensure input is a tensor\n",
    "    with torch.no_grad():\n",
    "        return model(input_data.view(-1, 3, 224, 224)).numpy()  # Reshape back to original image size\n",
    "\n",
    "# Initialize the Kernel Explainer with flattened images\n",
    "explainer = shap.KernelExplainer(model_predict, images_reshaped.numpy())\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(images_reshaped.numpy())\n",
    "\n",
    "# Visualize SHAP values for the first image\n",
    "shap.initjs()\n",
    "\n",
    "# Remove the batch dimension for visualization\n",
    "images_for_plotting = images.squeeze(0).permute(1, 2, 0).numpy()  # Shape to (224, 224, 3)\n",
    "\n",
    "shap.image_plot(shap_values, images_for_plotting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP based on each KP\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/example_notebooks/image_examples/image_classification/Explain%20ResNet50%20using%20the%20Partition%20explainer.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('keypoint_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Function to load dataset from a specified folder\n",
    "def load_dataset(folder_path):\n",
    "    # Load key points from the CSV file\n",
    "    csv_file = os.path.join(folder_path, 'augmented_labels.csv')  # Change to your CSV file name\n",
    "    keypoint_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create lists for images and key points\n",
    "    image_paths = []\n",
    "    key_points = []\n",
    "\n",
    "    # Iterate through the CSV to get paths and key points\n",
    "    for index, row in keypoint_data.iterrows():\n",
    "        image_name = row['image_name']  # Replace with the actual column name in your CSV\n",
    "        keypoint = row[1:].values.astype(float)  # Assuming the first column is the image name\n",
    "        image_path = os.path.join(folder_path, f\"{image_name}\")  # Assuming images are in PNG format\n",
    "\n",
    "        image_paths.append(image_path)\n",
    "        key_points.append(torch.tensor(keypoint))\n",
    "\n",
    "    return image_paths, key_points\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to model's input size\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize for ResNet\n",
    "    ])\n",
    "    \n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        image = Image.open(path).convert('RGB')  # Load image and convert to RGB\n",
    "        image = transform(image)  # Apply transformations\n",
    "        images.append(image)\n",
    "    \n",
    "    return torch.stack(images)  # Stack images into a single tensor\n",
    "\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Paths to your train and test folders\n",
    "train_folder = os.path.join(base_path, 'train', 'augmented_data')\n",
    "test_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Load datasets\n",
    "train_image_paths, train_key_points = load_dataset(train_folder)\n",
    "test_image_paths, test_key_points = load_dataset(test_folder)\n",
    "\n",
    "# Load and preprocess images\n",
    "X_train = load_and_preprocess_images(train_image_paths)\n",
    "X_test = load_and_preprocess_images(test_image_paths)\n",
    "\n",
    "# Convert key points to tensors\n",
    "y_train = torch.stack(train_key_points)\n",
    "y_test = torch.stack(test_key_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Assuming 'X_train' is your training data\n",
    "# Ensure that X_train is a PyTorch tensor and move it to the GPU\n",
    "X_train_tensor = torch.tensor(X_train).to(device)\n",
    "\n",
    "# Use a very small sample of data to avoid OOM error\n",
    "X_train_sample = X_train_tensor[:10]  # Adjust based on your needs\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Create the SHAP explainer\n",
    "explainer = shap.GradientExplainer(model.cpu(), X_train_sample.cpu())  # Use CPU for SHAP\n",
    "\n",
    "# Use torch.no_grad() to save memory during SHAP value computation\n",
    "with torch.no_grad():\n",
    "    shap_values = explainer.shap_values(X_train_sample.cpu())\n",
    "\n",
    "# Convert SHAP values back to numpy if needed\n",
    "shap_values_np = [val.detach().numpy() for val in shap_values]\n",
    "\n",
    "# Now you can use shap_values_np for further analysis or visualization\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'X_test' is your test data for which you want explanations\n",
    "shap_values = explainer.shap_values(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values in smaller batches\n",
    "shap_values = []\n",
    "batch_size = 1  # Adjust this value based on your available memory\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_test = X_test.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, X_test.shape[0], batch_size):\n",
    "        batch_shap_values = explainer.shap_values(X_test[i:i + batch_size])\n",
    "        shap_values.extend(batch_shap_values)\n",
    "        torch.cuda.empty_cache()  # Clear memory after each batch if necessary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):  # Loop through each key point\n",
    "    shap.initjs()\n",
    "    shap.summary_plot(shap_values[i], X_test, feature_names=your_feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "\n",
    "# Load the trained model\n",
    "model = KeyPointModel()\n",
    "model.load_state_dict(torch.load('R50_model_shap2.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "preprocessed_image = X_test[0]\n",
    "# Load a sample image or batch of images (ensure the preprocessing is the same as in training)\n",
    "sample_image = torch.tensor(preprocessed_image).unsqueeze(0)  # add batch dimension if needed\n",
    "\n",
    "print(sample_image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Wrapper(torch.nn.Module):\n",
    "    def __init__(self, model, keypoint_idx):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.keypoint_idx = keypoint_idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        # Extracting specific keypoint (e.g., keypoint 1 corresponds to output 0 and 1 for x and y)\n",
    "        return output[:, self.keypoint_idx*2:self.keypoint_idx*2+2]\n",
    "\n",
    "# Wrap the model to focus on a specific key point, e.g., keypoint 0 (x1, y1)\n",
    "wrapped_model = ResNet50Wrapper(model, keypoint_idx=0)  # Adjust keypoint_idx for different keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Gradient Explainer requires the model and the background dataset (a set of representative images)\n",
    "#background = torch.randn(50, 3, 224, 224)  # Random background data (replace with actual representative images if available)\n",
    "background = X_train[:100]\n",
    "# Create a SHAP Gradient explainer\n",
    "explainer = shap.GradientExplainer(wrapped_model, background)\n",
    "\n",
    "# Compute SHAP values for the keypoint of interest (e.g., keypoint 0)\n",
    "shap_values = explainer.shap_values(sample_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "background = X_train[:100].to(device)\n",
    "test_image = X_test[0].to(device)\n",
    "\n",
    "e = shap.DeepExplainer(model, background)\n",
    "shap_values = e.shap_values(test_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you want to visualize the first keypoint\n",
    "keypoint_index = 0  # Change this index to visualize a different keypoint\n",
    "\n",
    "# Extract the SHAP values for the specific keypoint\n",
    "shap_values_keypoint = shap_values[keypoint_index]  # This will have shape (224, 224, 2)\n",
    "\n",
    "# Since shap_values_keypoint is already in (H, W, 2), we can directly get the first channel (x or y) for visualization\n",
    "shap_values_x = shap_values_keypoint[..., 0]  # Shape (224, 224)\n",
    "shap_values_y = shap_values_keypoint[..., 1]  # Shape (224, 224)\n",
    "\n",
    "# Convert the sample image to a NumPy array with the correct shape (H, W, C)\n",
    "sample_image_numpy = sample_image.squeeze(0).detach().cpu().numpy().transpose(1, 2, 0)\n",
    "\n",
    "# Ensure the values are in the valid range for images (e.g., 0-255)\n",
    "sample_image_numpy = (sample_image_numpy * 255).astype(np.uint8)  # Assuming normalization was done earlier\n",
    "\n",
    "# Visualize the SHAP values for the selected keypoint's x coordinate as a heatmap overlay\n",
    "shap.image_plot([shap_values_x], sample_image_numpy)\n",
    "\n",
    "# If you want to visualize the y coordinate as well\n",
    "shap.image_plot([shap_values_y], sample_image_numpy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start over\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import json\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = ResNet50(weights='imagenet')\n",
    "def f(X):\n",
    "    tmp = X.copy()\n",
    "    preprocess_input(tmp)\n",
    "    return model(tmp)\n",
    "X, y = shap.datasets.imagenet50()\n",
    "\n",
    "# load the ImageNet class names as a vectorized mapping function from ids to names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "with open(shap.datasets.cache(url)) as file:\n",
    "    class_names = [v[1] for v in json.load(file).values()]\n",
    "\n",
    "# define a masker that is used to mask out partitions of the input image, this one uses a blurred background\n",
    "masker = shap.maskers.Image(\"inpaint_telea\", X[0].shape)\n",
    "\n",
    "# By default the Partition explainer is used for all  partition explainer\n",
    "explainer = shap.Explainer(f, masker, output_names=class_names)\n",
    "\n",
    "# here we use 500 evaluations of the underlying model to estimate the SHAP values\n",
    "shap_values = explainer(X[1:3], max_evals=500, batch_size=50, outputs=shap.Explanation.argsort.flip[:1])\n",
    "shap.image_plot(shap_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
