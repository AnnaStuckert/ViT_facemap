{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this markdown I have tried to just compile everything related to training a R50 model and calculating the SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "class ResNet50KeypointModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50KeypointModel, self).__init__()\n",
    "        # Load the pre-trained ResNet50 model\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        # Modify the final layer to output 24 values (12 key points with x and y coordinates)\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 24) \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "model = ResNet50KeypointModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset Definition\n",
    "class KeyPointDataset(Dataset):\n",
    "    def __init__(self, image_paths, key_points, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.key_points = key_points\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")  # Ensure images are in RGB format\n",
    "        key_point = self.key_points[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, key_point\n",
    "\n",
    "# Function to load dataset from a specified folder\n",
    "def load_dataset(folder_path):\n",
    "    # Load key points from the CSV file\n",
    "    csv_file = os.path.join(folder_path, 'augmented_labels.csv')  # Change to your CSV file name\n",
    "    keypoint_data = pd.read_csv(csv_file)\n",
    "\n",
    "    # Create lists for images and key points\n",
    "    image_paths = []\n",
    "    key_points = []\n",
    "\n",
    "    # Iterate through the CSV to get paths and key points\n",
    "    for index, row in keypoint_data.iterrows():\n",
    "        image_name = row['image_name']  # Replace with the actual column name in your CSV\n",
    "        keypoint = row[1:].values.astype(float)  # Assuming the first column is the image name\n",
    "        image_path = os.path.join(folder_path, f\"{image_name}\")  # Assuming images are in PNG format\n",
    "\n",
    "        image_paths.append(image_path)\n",
    "        key_points.append(torch.tensor(keypoint, dtype=torch.float32))  # Ensure keypoints are tensors of floats\n",
    "\n",
    "    return image_paths, key_points\n",
    "\n",
    "# Base path for your project\n",
    "base_path = os.path.expanduser('~/Documents/GitHub/ViT_facemap/ViT-pytorch/projects/Facemap/data')\n",
    "\n",
    "# Paths to your train and test folders\n",
    "train_folder = os.path.join(base_path, 'train', 'augmented_data')\n",
    "test_folder = os.path.join(base_path, 'test', 'augmented_data')\n",
    "\n",
    "# Load datasets\n",
    "train_image_paths, train_key_points = load_dataset(train_folder)\n",
    "test_image_paths, test_key_points = load_dataset(test_folder)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to fit ResNet input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = KeyPointDataset(train_image_paths, train_key_points, transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "\n",
    "test_dataset = KeyPointDataset(test_image_paths, test_key_points, transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the model, move it to the GPU, and set it to training mode\n",
    "model = ResNet50KeypointModel().to(device)\n",
    "model.train()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Adjust as necessary\n",
    "for epoch in range(num_epochs):\n",
    "    for img_batch, coord_batch in train_dataloader:\n",
    "        # Move data to the GPU\n",
    "        img_batch = img_batch.to(device)\n",
    "        coord_batch = coord_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(img_batch)  # Forward pass\n",
    "        loss = criterion(outputs, coord_batch)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    # Save the trained model\n",
    "    model_path = 'R50_model_shap2.pth'\n",
    "    torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP value visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model data\n",
    "\n",
    "model = ResNet50KeypointModel()\n",
    "model.load_state_dict(torch.load('R50_model_shap2.pth', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP explainer with my data\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "import requests\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import label2rgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# Load an image\n",
    "file = \"data/cam1_G7c1_1_img0076_rescale_augmented.jpg\"\n",
    "img = image.load_img(file, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)  # Convert to NumPy array directly here\n",
    "img_orig = image.img_to_array(img)\n",
    "\n",
    "# Segment the image so we don't have to explain every pixel\n",
    "segments_slic = slic(img_array.astype('uint8'), n_segments=50, compactness=30, sigma=3)\n",
    "\n",
    "# Create a color representation of the segments\n",
    "#segmented_image = label2rgb(segments_slic, img_array.astype('uint8'), kind='avg')\n",
    "segmented_image = segments_slic\n",
    "\n",
    "# Display the original image and the segmented image\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img_array.astype('uint8'))  # Normalize for display\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(segmented_image)\n",
    "plt.title(\"Segmented Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to mask the image\n",
    "def mask_image(zs, segmentation, image, background=None):\n",
    "    if background is None:\n",
    "        background = image.mean((0, 1))\n",
    "    out = np.zeros((zs.shape[0], image.shape[0], image.shape[1], image.shape[2]))\n",
    "    for i in range(zs.shape[0]):\n",
    "        out[i, :, :, :] = image\n",
    "        for j in range(zs.shape[1]):\n",
    "            if zs[i, j] == 0:\n",
    "                out[i][segmentation == j, :] = background\n",
    "    return out\n",
    "\n",
    "\n",
    "# Define a function for SHAP to call the model\n",
    "def f(z):\n",
    "    masked_image = mask_image(z, segments_slic, img_array)  # Create the masked image\n",
    "    masked_image = preprocess_input(masked_image)  # Preprocess the masked image\n",
    "    masked_image_tensor = torch.from_numpy(masked_image.copy()).permute(0, 3, 1, 2).float()  # Make a copy and rearrange dimensions\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        return model(masked_image_tensor).numpy()  # Forward pass and convert to numpy\n",
    "\n",
    "# Use Kernel SHAP to explain the network's predictions\n",
    "explainer = shap.KernelExplainer(f, np.zeros((1, 50)))  # Initialize SHAP explainer with dummy data. I believe this dummy data used in the KERNEL SHAP instance in Github, is what in other SHAP explainers is the 'background', which can be your training images. But this is what they used in the github example, so I guess it should be valid enough.\n",
    "shap_values = explainer.shap_values(np.ones((1, 50)), nsamples=200)  # Run SHAP with sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "img_orig = image.img_to_array(img)\n",
    "\n",
    "# Convert the image to a tensor and preprocess\n",
    "img_tensor = torch.from_numpy(img_orig).permute(2, 0, 1).unsqueeze(0).float()  # Convert to tensor and reorder dimensions\n",
    "\n",
    "# Define the preprocessing (normalization for ResNet50)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the tensor (this assumes the image is already in float32 format)\n",
    "img_tensor = preprocess(img_tensor[0]).unsqueeze(0)\n",
    "\n",
    "# Move to the correct device if needed (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Get the top predictions from the model\n",
    "model = model.to(device)  # Make sure the model is on the same device\n",
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Convert the image to a tensor and preprocess\n",
    "img_tensor = torch.from_numpy(img_orig).permute(2, 0, 1).unsqueeze(0).float()  # Convert to tensor and reorder dimensions\n",
    "\n",
    "# Define the preprocessing (normalization for ResNet50)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the tensor (this assumes the image is already in float32 format)\n",
    "img_tensor = preprocess(img_tensor[0]).unsqueeze(0)\n",
    "\n",
    "# Move to the correct device if needed (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Get the predictions from the model\n",
    "model = model.to(device)  # Make sure the model is on the same device\n",
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)\n",
    "\n",
    "# Instead of top predictions, we will visualize all 24 categories\n",
    "num_categories = 24  # Assuming 24 output categories\n",
    "\n",
    "# make a color map\n",
    "colors = []\n",
    "for l in np.linspace(1, 0, 100):\n",
    "    colors.append((245/255, 39/255, 87/255, l))\n",
    "for l in np.linspace(0, 1, 100):\n",
    "    colors.append((24/255, 196/255, 93/255, l))\n",
    "cm = LinearSegmentedColormap.from_list(\"shap\", colors)\n",
    "\n",
    "def fill_segmentation(values, segmentation):\n",
    "    out = np.zeros(segmentation.shape)\n",
    "    for i in range(len(values)):\n",
    "        out[segmentation == i] = values[i]\n",
    "    return out\n",
    "\n",
    "# Plot SHAP explanations for all output categories\n",
    "fig, axes = pl.subplots(nrows=4, ncols=6, figsize=(18, 12))  # 4x6 grid for 24 categories\n",
    "\n",
    "# Find max SHAP value for color scaling\n",
    "max_val = np.max([np.max(np.abs(shap_values[i][:, :-1])) for i in range(len(shap_values))])\n",
    "\n",
    "# Variable for storing the last valid imshow result\n",
    "im = None\n",
    "\n",
    "for i in range(num_categories):\n",
    "    row = i // 6\n",
    "    col = i % 6\n",
    "    ax = axes[row, col]  # Get the corresponding subplot\n",
    "    \n",
    "    # Ensure the index is within bounds for shap_values\n",
    "    if i < shap_values.shape[2]:  # Check if index is valid for SHAP values\n",
    "        m = fill_segmentation(shap_values[0][:, i], segments_slic)  # Use shap_values[0] for a single prediction\n",
    "        \n",
    "        # Display the SHAP values overlayed on the image\n",
    "        ax.imshow(img.convert('LA'), alpha=0.15)\n",
    "        im = ax.imshow(m, cmap='jet', vmin=-max_val, vmax=max_val)  # Specify your colormap here\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Category {i+1}')  # You can customize titles based on category names\n",
    "    else:\n",
    "        print(f\"Index {i} is out of bounds for shap_values with shape {shap_values.shape}\")\n",
    "\n",
    "# Create a color bar only if `im` has been defined\n",
    "if im is not None:\n",
    "    cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
    "    cb.outline.set_visible(False)\n",
    "\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Convert the image to a tensor and preprocess\n",
    "img_tensor = torch.from_numpy(img_orig).permute(2, 0, 1).unsqueeze(0).float()  # Convert to tensor and reorder dimensions\n",
    "\n",
    "# Define the preprocessing (normalization for ResNet50)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply preprocessing to the tensor (this assumes the image is already in float32 format)\n",
    "img_tensor = preprocess(img_tensor[0]).unsqueeze(0)\n",
    "\n",
    "# Move to the correct device if needed (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "img_tensor = img_tensor.to(device)\n",
    "\n",
    "# Get the predictions from the model\n",
    "model = model.to(device)  # Make sure the model is on the same device\n",
    "with torch.no_grad():\n",
    "    preds = model(img_tensor)\n",
    "\n",
    "# Instead of top predictions, we will visualize all 24 categories\n",
    "num_categories = 24  # Assuming 24 output categories\n",
    "\n",
    "# make a color map\n",
    "colors = []\n",
    "for l in np.linspace(1, 0, 100):\n",
    "    colors.append((245/255, 39/255, 87/255, l))\n",
    "for l in np.linspace(0, 1, 100):\n",
    "    colors.append((24/255, 196/255, 93/255, l))\n",
    "cm = LinearSegmentedColormap.from_list(\"shap\", colors)\n",
    "\n",
    "def fill_segmentation(values, segmentation):\n",
    "    out = np.zeros(segmentation.shape)\n",
    "    for i in range(len(values)):\n",
    "        out[segmentation == i] = values[i]\n",
    "    return out\n",
    "\n",
    "# Plot SHAP explanations for all output categories\n",
    "fig, axes = pl.subplots(nrows=4, ncols=6, figsize=(18, 12))  # 4x6 grid for 24 categories\n",
    "\n",
    "# Find max SHAP value for color scaling\n",
    "max_val = np.max([np.max(np.abs(shap_values[i][:, :-1])) for i in range(len(shap_values))])\n",
    "\n",
    "# Variable for storing the last valid imshow result\n",
    "im = None\n",
    "\n",
    "for i in range(num_categories):\n",
    "    row = i // 6\n",
    "    col = i % 6\n",
    "    ax = axes[row, col]  # Get the corresponding subplot\n",
    "    \n",
    "    # Ensure the index is within bounds for shap_values\n",
    "    if i < shap_values.shape[2]:  # Check if index is valid for SHAP values\n",
    "        m = fill_segmentation(shap_values[0][:, i], segments_slic)  # Use shap_values[0] for a single prediction\n",
    "        \n",
    "        # Display the SHAP values overlayed on the original RGB image\n",
    "        ax.imshow(img, alpha=1.0)  # Show original image\n",
    "        im = ax.imshow(m, cmap='jet', vmin=-max_val, vmax=max_val, alpha=0.5)  # Overlay SHAP values with transparency\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'Category {i+1}')  # You can customize titles based on category names\n",
    "    else:\n",
    "        print(f\"Index {i} is out of bounds for shap_values with shape {shap_values.shape}\")\n",
    "\n",
    "# Create a color bar only if `im` has been defined\n",
    "if im is not None:\n",
    "    cb = fig.colorbar(im, ax=axes.ravel().tolist(), label=\"SHAP value\", orientation=\"horizontal\", aspect=60)\n",
    "    cb.outline.set_visible(False)\n",
    "\n",
    "pl.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
